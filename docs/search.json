[
  {
    "objectID": "posts/hello/index.html",
    "href": "posts/hello/index.html",
    "title": "1",
    "section": "",
    "text": "hi"
  },
  {
    "objectID": "posts/hello/index.html#greeting-message",
    "href": "posts/hello/index.html#greeting-message",
    "title": "Hello",
    "section": "",
    "text": "To start, let’s write a simple greeting message using Quarto:"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "nimwver",
    "section": "",
    "text": "ML 개발자를 목표로 공부하고 있습니다. 하지만 공부하고 있지 않는 시간이 많습니다. 이런!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "web+pyramid",
    "section": "",
    "text": "아이돌 얼굴 인식 - InsightFace\n\n\n\n\n\n\ncode\n\n\njupyter\n\n\nInsightFace\n\n\n\n임베딩을 활용한 얼굴 인식 시스템을 만들어봅니다.\n\n\n\n\n\nJul 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n아이돌 얼굴 인식 - ResNet\n\n\n\n\n\n\ncode\n\n\njupyter\n\n\n\n여러 측면에서 활용 가능한 얼굴 인식 시스템을 만들고 탐구해봅니다.\n\n\n\n\n\nJul 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMNIST 숫자 분류\n\n\n\n\n\n\ncode\n\n\njupyter\n\n\n\n\n\n\n\n\n\nMay 25, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hello/index.html#section",
    "href": "posts/hello/index.html#section",
    "title": "1",
    "section": "",
    "text": "hi"
  },
  {
    "objectID": "posts/hello/hello.html",
    "href": "posts/hello/hello.html",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "PyTorch에서 불러온 모델을 머신러닝에서 가장 유명한 MNIST 손글씨 숫자 데이터 세트의 숫자를 올바르게 분류하는 모델로 만들어 봅니다.\n\n\nMNIST 손글씨 숫자 데이터 세트를 불러오는 방법에는 여러 가지가 있습니다. 간단한 방법으로는 Keras나 TensorFlow-datasets, Scikit-Learn을 통해 로드하는 방법 등이 있지만, 그래도 가장 쉬운건 역시 직접 다운로드하는 방법입니다.\n여기선 가장 일반적인 방법은 아니지만 torchvision을 통해 데이터 세트를 다운로드하도록 하겠습니다. 참고로 해당 코드에서는 이 후 PyTorch 사용을 위해 데이터 세트는 다운로드와 동시에 텐서로 변환하겠습니다.\n\nfrom torchvision import datasets, transforms\n\ndef load_mnist(root='./data', download=True, transform=transforms.ToTensor()):\n    return (\n        datasets.MNIST(root=root, train=True, download=download, transform=transform),\n        datasets.MNIST(root=root, train=False, download=download, transform=transform)\n    )\n\nmnist_train, mnist_test = load_mnist()\n\n로드된 이미지는 다음과 같이 확인할 수 있습니다. 훈련 세트와 테스트 세트 각 첫 번째 이미지를 확인해 보겠습니다.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nim_trn,lb_trn = mnist_train[0]\nim_tst,lb_tst = mnist_test[0]\n\n# 시각화를 위해 텐서를 28x28 numpy배열로 재변환\nim_trn = im_trn.numpy().reshape(28, 28)\nim_tst = im_tst.numpy().reshape(28, 28)\n\nfig, axs = plt.subplots(1, 2, figsize=(4, 2))\n\naxs[0].imshow(im_trn, cmap='gray')\naxs[0].set_title(lb_trn)\n\naxs[1].imshow(im_tst, cmap='gray')\naxs[1].set_title(lb_tst)\n\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n훈련 세트와 검증 세트를 분리한 후, 훈련 시 데이터들을 불러오는 데이터 로더를 정의해 보겠습니다. 훈련 데이터 세트의 80%를 훈련 세트인 dls로, 나머지를 하나의 에포크가 끝난 후 평가하는 val_dls로 분리합니다. 테스트 데이터 세트는 모델의 마지막 평가를 위해 남겨둡니다. 전부 PyTorch의 모듈을 이용합니다.\n참고로 배치 사이즈는 2의 지수 형태가 일반적이라고 하네요.\n\nfrom torch.utils.data import DataLoader, random_split\n\nbatch_size = 128\ntrain_size = int(0.8 * len(mnist_train))\nval_size = len(mnist_train) - train_size\n\ntrain_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n\ndls = DataLoader(dataset=train_dataset,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 drop_last=True)\n\nval_dls = DataLoader(dataset=val_dataset,\n                     batch_size=batch_size,\n                     shuffle=True,\n                     drop_last=True)\n\ntst_dls = DataLoader(dataset=mnist_test,\n                     batch_size=batch_size,\n                     shuffle=False)\n\n\n\n\n다음은 torchvision의 models 모듈을 통해 모델(‘ResNet-18’)을 정의합니다. 주의할 점은 MNIST 이미지는 흑백(grayscale) 이미지이기 때문에 이미지 입력 레이어의 채널이 3(RGB)이 아닌 1이 되어야 한다는 것입니다. 또한 이 모델은 분류 모델이므로 출력의 종류를 정의하는 선형 레이어를 추가해야합니다.\n\n\nOriginal: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nAfter: Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n미세 조정(Fine-tuning)을 하고 싶다면 모델을 불러오는 과정에서 pretrained 인자를 True로 하고 이미 학습된 모델의 가중치와 편향 등의 매개변수(parameter)를 고정할 수 있게 모델 매개변수의 requires_grad를 False로 바꿔주면 되니 참고하시기 바랍니다.\n\nimport torch\nfrom torchvision import models\n\nn_classes = 10\n\nmodel = models.resnet18(pretrained=False)\n\n# 미세 조정 시 모델의 매개변수를 고정(freeze)\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# 모델의 첫 번째 계층을 수정\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# 출력 계층 수정\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)\n\n\n\n\n손실 함수와 최적화 함수를 정의합니다. 손실 함수는 분류 모델에 쓰이는 Cross Entropy Loss를 사용하고, 최적화 함수엔 Adam을 사용합니다. 스케줄러를 통해 최적화 함수의 속도를 조절합니다. 5 에포크마다 학습률에 감마(0.1)값을 곱합니다.\n\ncriterion = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 15\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n\nGPU 설정을 위해 device 변수를 정의합니다. Nvidia GPU를 사용할 수 있는 환경이라면 ’cuda’를, 그 외에는 ’cpu’를 사용합니다. GPU에서 훈련시키기 위해서는 훈련에 필요한 모든 변수를 GPU로 이동시켜야 합니다. 이를 위해 .to(device)를 사용해 모델을 옮깁니다. 아래 훈련 부분에서도 반복하여 사용됩니다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\n\n\n\n훈련 루프를 작성합니다. train 모드와 eval 모드로 나누어 한 에포크의 훈련이 끝날 때 마다 검증 세트로 성능을 평가합니다. 설정한 에포크 수 만큼 훈련합니다.\n성능 평가 시엔 손실도 같이 보는 경우도 많지만, 이를 확인하는 방법 또한 정확도 계산과 크게 차이가 없습니다. 손실을 구하는 방법은 주석으로 달아놨으니 참고하시면 되겠습니다.\n\nfor epoch in range(n_epochs):\n    \n    model.train() # 모델 훈련\n    # loss_epoch = 0. -&gt; 손실 계산\n    for inputs, labels in dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optim.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optim.step()\n\n        # loss_epoch += loss.item()\n\n    # avg_loss = loss_epoch / len(dls) -&gt; 에포크 당 평균 손실\n\n    model.eval() # 모델 평가\n    with torch.no_grad():\n        total = 0\n        correct = 0\n        for inputs, labels in val_dls:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Epoch: {epoch}, Accuracy on validation set: {100. * correct / total:.2f}')\n\nEpoch: 0, Accuracy on validation set: 96 %\nEpoch: 1, Accuracy on validation set: 97 %\nEpoch: 2, Accuracy on validation set: 98 %\nEpoch: 3, Accuracy on validation set: 98 %\nEpoch: 4, Accuracy on validation set: 97 %\nEpoch: 5, Accuracy on validation set: 98 %\nEpoch: 6, Accuracy on validation set: 98 %\nEpoch: 7, Accuracy on validation set: 98 %\nEpoch: 8, Accuracy on validation set: 98 %\nEpoch: 9, Accuracy on validation set: 98 %\nEpoch: 10, Accuracy on validation set: 98 %\nEpoch: 11, Accuracy on validation set: 98 %\n\n\nKeyboardInterrupt: \n\n\n정확도를 확인하며 적당한 시점에 훈련을 멈추는 것도 괜찮습니다.\n\n\n\n\nmodel.eval()\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for inputs, labels in tst_dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total_samples += labels.size(0)\n        total_correct += (predicted == labels).sum().item()\n\naccuracy = 100. * total_correct / total_samples\nprint(f'Accuracy on test set: {accuracy}%')\n\nAccuracy on test set: 99.13%\n\n\n테스트 세트 기준, 이 모델은 99% 정도의 정확도로 MNIST 데이터 셋을 올바르게 분류할 수 있는 것을 확인할 수 있었습니다.\n\n이렇게 비교적 간단한 방법으로 MNIST 손글씨 숫자 데이터 셋을 분류하는 모델을 만들어 봤습니다. 기초부터 만드는 방법도 있지만 ResNet과 같은 모델을 사용하면 시간을 크게 들이지 않고 정확도가 높은 모델을 만들어 낼 수 있습니다.\n다음 시간에도 엔지니어링 관점에서 머신러닝을 잘 활용할 수 있는 포스트로 만나뵙겠습니다.\nCiao!"
  },
  {
    "objectID": "posts/hello/hello.html#mnist-load",
    "href": "posts/hello/hello.html#mnist-load",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "MNIST 손글씨 숫자 데이터 세트를 불러오는 방법에는 여러 가지가 있습니다. 간단한 방법으로는 Keras나 TensorFlow-datasets, Scikit-Learn을 통해 로드하는 방법 등이 있지만, 그래도 가장 쉬운건 역시 직접 다운로드하는 방법입니다.\n여기선 가장 일반적인 방법은 아니지만 torchvision을 통해 데이터 세트를 다운로드하도록 하겠습니다. 참고로 해당 코드에서는 이 후 PyTorch 사용을 위해 데이터 세트는 다운로드와 동시에 텐서로 변환하겠습니다.\n\nfrom torchvision import datasets, transforms\n\ndef load_mnist(root='./data', download=True, transform=transforms.ToTensor()):\n    return (\n        datasets.MNIST(root=root, train=True, download=download, transform=transform),\n        datasets.MNIST(root=root, train=False, download=download, transform=transform)\n    )\n\nmnist_train, mnist_test = load_mnist()\n\n로드된 이미지는 다음과 같이 확인할 수 있습니다. 훈련 세트와 테스트 세트 각 첫 번째 이미지를 확인해 보겠습니다.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nim_trn,lb_trn = mnist_train[0]\nim_tst,lb_tst = mnist_test[0]\n\n# 시각화를 위해 텐서를 28x28 numpy배열로 재변환\nim_trn = im_trn.numpy().reshape(28, 28)\nim_tst = im_tst.numpy().reshape(28, 28)\n\nfig, axs = plt.subplots(1, 2, figsize=(4, 2))\n\naxs[0].imshow(im_trn, cmap='gray')\naxs[0].set_title(lb_trn)\n\naxs[1].imshow(im_tst, cmap='gray')\naxs[1].set_title(lb_tst)\n\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/hello/hello.html#데이터-로더",
    "href": "posts/hello/hello.html#데이터-로더",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "훈련 세트와 검증 세트를 분리한 후, 훈련 시 데이터들을 불러오는 데이터 로더를 정의해 보겠습니다. 훈련 데이터 세트의 80%를 훈련 세트인 dls로, 나머지를 하나의 에포크가 끝난 후 평가하는 val_dls로 분리합니다. 테스트 데이터 세트는 모델의 마지막 평가를 위해 남겨둡니다. 전부 PyTorch의 모듈을 이용합니다.\n참고로 배치 사이즈는 2의 지수 형태가 일반적이라고 하네요.\n\nfrom torch.utils.data import DataLoader, random_split\n\nbatch_size = 128\ntrain_size = int(0.8 * len(mnist_train))\nval_size = len(mnist_train) - train_size\n\ntrain_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n\ndls = DataLoader(dataset=train_dataset,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 drop_last=True)\n\nval_dls = DataLoader(dataset=val_dataset,\n                     batch_size=batch_size,\n                     shuffle=True,\n                     drop_last=True)\n\ntst_dls = DataLoader(dataset=mnist_test,\n                     batch_size=batch_size,\n                     shuffle=False)"
  },
  {
    "objectID": "posts/hello/hello.html#모델-정의",
    "href": "posts/hello/hello.html#모델-정의",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "다음은 torchvision의 models 모듈을 통해 모델(‘ResNet-18’)을 정의합니다. 주의할 점은 MNIST 이미지는 흑백(grayscale) 이미지이기 때문에 이미지 입력 레이어의 채널이 3(RGB)이 아닌 1이 되어야 한다는 것입니다. 또한 이 모델은 분류 모델이므로 출력의 종류를 정의하는 선형 레이어를 추가해야합니다.\n\n\nOriginal: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nAfter: Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n미세 조정(Fine-tuning)을 하고 싶다면 모델을 불러오는 과정에서 pretrained 인자를 True로 하고 이미 학습된 모델의 가중치와 편향 등의 매개변수(parameter)를 고정할 수 있게 모델 매개변수의 requires_grad를 False로 바꿔주면 되니 참고하시기 바랍니다.\n\nimport torch\nfrom torchvision import models\n\nn_classes = 10\n\nmodel = models.resnet18(pretrained=False)\n\n# 미세 조정 시 모델의 매개변수를 고정(freeze)\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# 모델의 첫 번째 계층을 수정\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# 출력 계층 수정\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)"
  },
  {
    "objectID": "posts/hello/hello.html#손실-함수와-최적화-함수-cuda",
    "href": "posts/hello/hello.html#손실-함수와-최적화-함수-cuda",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "손실 함수와 최적화 함수를 정의합니다. 손실 함수는 분류 모델에 쓰이는 Cross Entropy Loss를 사용하고, 최적화 함수엔 Adam을 사용합니다. 스케줄러를 통해 최적화 함수의 속도를 조절합니다. 5 에포크마다 학습률에 감마(0.1)값을 곱합니다.\n\ncriterion = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 15\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n\nGPU 설정을 위해 device 변수를 정의합니다. Nvidia GPU를 사용할 수 있는 환경이라면 ’cuda’를, 그 외에는 ’cpu’를 사용합니다. GPU에서 훈련시키기 위해서는 훈련에 필요한 모든 변수를 GPU로 이동시켜야 합니다. 이를 위해 .to(device)를 사용해 모델을 옮깁니다. 아래 훈련 부분에서도 반복하여 사용됩니다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)"
  },
  {
    "objectID": "posts/hello/hello.html#훈련",
    "href": "posts/hello/hello.html#훈련",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "훈련 루프를 작성합니다. train 모드와 eval 모드로 나누어 한 에포크의 훈련이 끝날 때 마다 검증 세트로 성능을 평가합니다. 설정한 에포크 수 만큼 훈련합니다.\n성능 평가 시엔 손실도 같이 보는 경우도 많지만, 이를 확인하는 방법 또한 정확도 계산과 크게 차이가 없습니다. 손실을 구하는 방법은 주석으로 달아놨으니 참고하시면 되겠습니다.\n\nfor epoch in range(n_epochs):\n    \n    model.train() # 모델 훈련\n    # loss_epoch = 0. -&gt; 손실 계산\n    for inputs, labels in dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optim.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optim.step()\n\n        # loss_epoch += loss.item()\n\n    # avg_loss = loss_epoch / len(dls) -&gt; 에포크 당 평균 손실\n\n    model.eval() # 모델 평가\n    with torch.no_grad():\n        total = 0\n        correct = 0\n        for inputs, labels in val_dls:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Epoch: {epoch}, Accuracy on validation set: {100. * correct / total:.2f}')\n\nEpoch: 0, Accuracy on validation set: 96 %\nEpoch: 1, Accuracy on validation set: 97 %\nEpoch: 2, Accuracy on validation set: 98 %\nEpoch: 3, Accuracy on validation set: 98 %\nEpoch: 4, Accuracy on validation set: 97 %\nEpoch: 5, Accuracy on validation set: 98 %\nEpoch: 6, Accuracy on validation set: 98 %\nEpoch: 7, Accuracy on validation set: 98 %\nEpoch: 8, Accuracy on validation set: 98 %\nEpoch: 9, Accuracy on validation set: 98 %\nEpoch: 10, Accuracy on validation set: 98 %\nEpoch: 11, Accuracy on validation set: 98 %\n\n\nKeyboardInterrupt: \n\n\n정확도를 확인하며 적당한 시점에 훈련을 멈추는 것도 괜찮습니다."
  },
  {
    "objectID": "posts/hello/hello.html#결과",
    "href": "posts/hello/hello.html#결과",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "model.eval()\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for inputs, labels in tst_dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total_samples += labels.size(0)\n        total_correct += (predicted == labels).sum().item()\n\naccuracy = 100. * total_correct / total_samples\nprint(f'Accuracy on test set: {accuracy}%')\n\nAccuracy on test set: 99.13%\n\n\n테스트 세트 기준, 이 모델은 99% 정도의 정확도로 MNIST 데이터 셋을 올바르게 분류할 수 있는 것을 확인할 수 있었습니다.\n\n이렇게 비교적 간단한 방법으로 MNIST 손글씨 숫자 데이터 셋을 분류하는 모델을 만들어 봤습니다. 기초부터 만드는 방법도 있지만 ResNet과 같은 모델을 사용하면 시간을 크게 들이지 않고 정확도가 높은 모델을 만들어 낼 수 있습니다.\n다음 시간에도 엔지니어링 관점에서 머신러닝을 잘 활용할 수 있는 포스트로 만나뵙겠습니다.\nCiao!"
  },
  {
    "objectID": "posts/hello/MNIST 숫자 분류.html",
    "href": "posts/hello/MNIST 숫자 분류.html",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "PyTorch에서 불러온 모델을 머신러닝에서 가장 유명한 MNIST 손글씨 숫자 데이터 세트의 숫자를 올바르게 분류하는 모델로 만들어 봅니다.\n\n\nMNIST 손글씨 숫자 데이터 세트를 불러오는 방법에는 여러 가지가 있습니다. 간단한 방법으로는 Keras나 TensorFlow-datasets, Scikit-Learn을 통해 로드하는 방법 등이 있지만, 그래도 가장 쉬운건 역시 직접 다운로드하는 방법입니다.\n여기선 가장 일반적인 방법은 아니지만 torchvision을 통해 데이터 세트를 다운로드하도록 하겠습니다. 참고로 해당 코드에서는 이 후 PyTorch 사용을 위해 데이터 세트는 다운로드와 동시에 텐서로 변환하겠습니다.\n\nfrom torchvision import datasets, transforms\n\ndef load_mnist(root='./data', download=True, transform=transforms.ToTensor()):\n    return (\n        datasets.MNIST(root=root, train=True, download=download, transform=transform),\n        datasets.MNIST(root=root, train=False, download=download, transform=transform)\n    )\n\nmnist_train, mnist_test = load_mnist()\n\n로드된 이미지는 다음과 같이 확인할 수 있습니다. 훈련 세트와 테스트 세트 각 첫 번째 이미지를 확인해 보겠습니다.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nim_trn,lb_trn = mnist_train[0]\nim_tst,lb_tst = mnist_test[0]\n\n# 시각화를 위해 텐서를 28x28 numpy배열로 재변환\nim_trn = im_trn.numpy().reshape(28, 28)\nim_tst = im_tst.numpy().reshape(28, 28)\n\nfig, axs = plt.subplots(1, 2, figsize=(4, 2))\n\naxs[0].imshow(im_trn, cmap='gray')\naxs[0].set_title(lb_trn)\n\naxs[1].imshow(im_tst, cmap='gray')\naxs[1].set_title(lb_tst)\n\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n훈련 세트와 검증 세트를 분리한 후, 훈련 시 데이터들을 불러오는 데이터 로더를 정의해 보겠습니다. 훈련 데이터 세트의 80%를 훈련 세트인 dls로, 나머지를 하나의 에포크가 끝난 후 평가하는 val_dls로 분리합니다. 테스트 데이터 세트는 모델의 마지막 평가를 위해 남겨둡니다. 전부 PyTorch의 모듈을 이용합니다.\n참고로 배치 사이즈는 2의 지수 형태가 일반적이라고 하네요.\n\nfrom torch.utils.data import DataLoader, random_split\n\nbatch_size = 128\ntrain_size = int(0.8 * len(mnist_train))\nval_size = len(mnist_train) - train_size\n\ntrain_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n\ndls = DataLoader(dataset=train_dataset,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 drop_last=True)\n\nval_dls = DataLoader(dataset=val_dataset,\n                     batch_size=batch_size,\n                     shuffle=True,\n                     drop_last=True)\n\ntst_dls = DataLoader(dataset=mnist_test,\n                     batch_size=batch_size,\n                     shuffle=False)\n\n\n\n\n다음은 torchvision의 models 모듈을 통해 모델(‘ResNet-18’)을 정의합니다. 주의할 점은 MNIST 이미지는 흑백(grayscale) 이미지이기 때문에 이미지 입력 레이어의 채널이 3(RGB)이 아닌 1이 되어야 한다는 것입니다. 또한 이 모델은 분류 모델이므로 출력의 종류를 정의하는 선형 레이어를 추가해야합니다.\n\n\nOriginal: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nAfter: Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n미세 조정(Fine-tuning)을 하고 싶다면 모델을 불러오는 과정에서 pretrained 인자를 True로 하고 이미 학습된 모델의 가중치와 편향 등의 매개변수(parameter)를 고정할 수 있게 모델 매개변수의 requires_grad를 False로 바꿔주면 되니 참고하시기 바랍니다.\n\nimport torch\nfrom torchvision import models\n\nn_classes = 10\n\nmodel = models.resnet18(pretrained=False)\n\n# 미세 조정 시 모델의 매개변수를 고정(freeze)\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# 모델의 첫 번째 계층을 수정\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# 출력 계층 수정\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)\n\n\n\n\n손실 함수와 최적화 함수를 정의합니다. 손실 함수는 분류 모델에 쓰이는 Cross Entropy Loss를 사용하고, 최적화 함수엔 Adam을 사용합니다. 스케줄러를 통해 최적화 함수의 속도를 조절합니다. 5 에포크마다 학습률에 감마(0.1)값을 곱합니다.\n\ncriterion = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 15\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n\nGPU 설정을 위해 device 변수를 정의합니다. Nvidia GPU를 사용할 수 있는 환경이라면 ’cuda’를, 그 외에는 ’cpu’를 사용합니다. GPU에서 훈련시키기 위해서는 훈련에 필요한 모든 변수를 GPU로 이동시켜야 합니다. 이를 위해 .to(device)를 사용해 모델을 옮깁니다. 아래 훈련 부분에서도 반복하여 사용됩니다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\n\n\n\n훈련 루프를 작성합니다. train 모드와 eval 모드로 나누어 한 에포크의 훈련이 끝날 때 마다 검증 세트로 성능을 평가합니다. 설정한 에포크 수 만큼 훈련합니다.\n성능 평가 시엔 손실도 같이 보는 경우도 많지만, 이를 확인하는 방법 또한 정확도 계산과 크게 차이가 없습니다. 손실을 구하는 방법은 주석으로 달아놨으니 참고하시면 되겠습니다.\n\nfor epoch in range(n_epochs):\n    \n    model.train() # 모델 훈련\n    # loss_epoch = 0. -&gt; 손실 계산\n    for inputs, labels in dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optim.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optim.step()\n\n        # loss_epoch += loss.item()\n\n    # avg_loss = loss_epoch / len(dls) -&gt; 에포크 당 평균 손실\n\n    model.eval() # 모델 평가\n    with torch.no_grad():\n        total = 0\n        correct = 0\n        for inputs, labels in val_dls:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Epoch: {epoch}, Accuracy on validation set: {100. * correct / total:.2f}')\n\nEpoch: 0, Accuracy on validation set: 96 %\nEpoch: 1, Accuracy on validation set: 97 %\nEpoch: 2, Accuracy on validation set: 98 %\nEpoch: 3, Accuracy on validation set: 98 %\nEpoch: 4, Accuracy on validation set: 97 %\nEpoch: 5, Accuracy on validation set: 98 %\nEpoch: 6, Accuracy on validation set: 98 %\nEpoch: 7, Accuracy on validation set: 98 %\nEpoch: 8, Accuracy on validation set: 98 %\nEpoch: 9, Accuracy on validation set: 98 %\nEpoch: 10, Accuracy on validation set: 98 %\nEpoch: 11, Accuracy on validation set: 98 %\n\n\nKeyboardInterrupt: \n\n\n정확도를 확인하며 적당한 시점에 훈련을 멈추는 것도 괜찮습니다.\n\n\n\n\nmodel.eval()\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for inputs, labels in tst_dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total_samples += labels.size(0)\n        total_correct += (predicted == labels).sum().item()\n\naccuracy = 100. * total_correct / total_samples\nprint(f'Accuracy on test set: {accuracy}%')\n\nAccuracy on test set: 99.13%\n\n\n테스트 세트 기준, 이 모델은 99% 정도의 정확도로 MNIST 데이터 셋을 올바르게 분류할 수 있는 것을 확인할 수 있었습니다.\n\n이렇게 비교적 간단한 방법으로 MNIST 손글씨 숫자 데이터 셋을 분류하는 모델을 만들어 봤습니다. 기초부터 만드는 방법도 있지만 ResNet과 같은 모델을 사용하면 시간을 크게 들이지 않고 정확도가 높은 모델을 만들어 낼 수 있습니다.\n다음 시간에도 엔지니어링 관점에서 머신러닝을 잘 활용할 수 있는 포스트로 만나뵙겠습니다.\nCiao!"
  },
  {
    "objectID": "posts/hello/MNIST 숫자 분류.html#mnist-load",
    "href": "posts/hello/MNIST 숫자 분류.html#mnist-load",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "MNIST 손글씨 숫자 데이터 세트를 불러오는 방법에는 여러 가지가 있습니다. 간단한 방법으로는 Keras나 TensorFlow-datasets, Scikit-Learn을 통해 로드하는 방법 등이 있지만, 그래도 가장 쉬운건 역시 직접 다운로드하는 방법입니다.\n여기선 가장 일반적인 방법은 아니지만 torchvision을 통해 데이터 세트를 다운로드하도록 하겠습니다. 참고로 해당 코드에서는 이 후 PyTorch 사용을 위해 데이터 세트는 다운로드와 동시에 텐서로 변환하겠습니다.\n\nfrom torchvision import datasets, transforms\n\ndef load_mnist(root='./data', download=True, transform=transforms.ToTensor()):\n    return (\n        datasets.MNIST(root=root, train=True, download=download, transform=transform),\n        datasets.MNIST(root=root, train=False, download=download, transform=transform)\n    )\n\nmnist_train, mnist_test = load_mnist()\n\n로드된 이미지는 다음과 같이 확인할 수 있습니다. 훈련 세트와 테스트 세트 각 첫 번째 이미지를 확인해 보겠습니다.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nim_trn,lb_trn = mnist_train[0]\nim_tst,lb_tst = mnist_test[0]\n\n# 시각화를 위해 텐서를 28x28 numpy배열로 재변환\nim_trn = im_trn.numpy().reshape(28, 28)\nim_tst = im_tst.numpy().reshape(28, 28)\n\nfig, axs = plt.subplots(1, 2, figsize=(4, 2))\n\naxs[0].imshow(im_trn, cmap='gray')\naxs[0].set_title(lb_trn)\n\naxs[1].imshow(im_tst, cmap='gray')\naxs[1].set_title(lb_tst)\n\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/hello/MNIST 숫자 분류.html#데이터-로더",
    "href": "posts/hello/MNIST 숫자 분류.html#데이터-로더",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "훈련 세트와 검증 세트를 분리한 후, 훈련 시 데이터들을 불러오는 데이터 로더를 정의해 보겠습니다. 훈련 데이터 세트의 80%를 훈련 세트인 dls로, 나머지를 하나의 에포크가 끝난 후 평가하는 val_dls로 분리합니다. 테스트 데이터 세트는 모델의 마지막 평가를 위해 남겨둡니다. 전부 PyTorch의 모듈을 이용합니다.\n참고로 배치 사이즈는 2의 지수 형태가 일반적이라고 하네요.\n\nfrom torch.utils.data import DataLoader, random_split\n\nbatch_size = 128\ntrain_size = int(0.8 * len(mnist_train))\nval_size = len(mnist_train) - train_size\n\ntrain_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n\ndls = DataLoader(dataset=train_dataset,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 drop_last=True)\n\nval_dls = DataLoader(dataset=val_dataset,\n                     batch_size=batch_size,\n                     shuffle=True,\n                     drop_last=True)\n\ntst_dls = DataLoader(dataset=mnist_test,\n                     batch_size=batch_size,\n                     shuffle=False)"
  },
  {
    "objectID": "posts/hello/MNIST 숫자 분류.html#모델-정의",
    "href": "posts/hello/MNIST 숫자 분류.html#모델-정의",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "다음은 torchvision의 models 모듈을 통해 모델(‘ResNet-18’)을 정의합니다. 주의할 점은 MNIST 이미지는 흑백(grayscale) 이미지이기 때문에 이미지 입력 레이어의 채널이 3(RGB)이 아닌 1이 되어야 한다는 것입니다. 또한 이 모델은 분류 모델이므로 출력의 종류를 정의하는 선형 레이어를 추가해야합니다.\n\n\nOriginal: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nAfter: Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n미세 조정(Fine-tuning)을 하고 싶다면 모델을 불러오는 과정에서 pretrained 인자를 True로 하고 이미 학습된 모델의 가중치와 편향 등의 매개변수(parameter)를 고정할 수 있게 모델 매개변수의 requires_grad를 False로 바꿔주면 되니 참고하시기 바랍니다.\n\nimport torch\nfrom torchvision import models\n\nn_classes = 10\n\nmodel = models.resnet18(pretrained=False)\n\n# 미세 조정 시 모델의 매개변수를 고정(freeze)\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# 모델의 첫 번째 계층을 수정\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# 출력 계층 수정\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)"
  },
  {
    "objectID": "posts/hello/MNIST 숫자 분류.html#손실-함수와-최적화-함수-cuda",
    "href": "posts/hello/MNIST 숫자 분류.html#손실-함수와-최적화-함수-cuda",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "손실 함수와 최적화 함수를 정의합니다. 손실 함수는 분류 모델에 쓰이는 Cross Entropy Loss를 사용하고, 최적화 함수엔 Adam을 사용합니다. 스케줄러를 통해 최적화 함수의 속도를 조절합니다. 5 에포크마다 학습률에 감마(0.1)값을 곱합니다.\n\ncriterion = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 15\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n\nGPU 설정을 위해 device 변수를 정의합니다. Nvidia GPU를 사용할 수 있는 환경이라면 ’cuda’를, 그 외에는 ’cpu’를 사용합니다. GPU에서 훈련시키기 위해서는 훈련에 필요한 모든 변수를 GPU로 이동시켜야 합니다. 이를 위해 .to(device)를 사용해 모델을 옮깁니다. 아래 훈련 부분에서도 반복하여 사용됩니다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)"
  },
  {
    "objectID": "posts/hello/MNIST 숫자 분류.html#훈련",
    "href": "posts/hello/MNIST 숫자 분류.html#훈련",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "훈련 루프를 작성합니다. train 모드와 eval 모드로 나누어 한 에포크의 훈련이 끝날 때 마다 검증 세트로 성능을 평가합니다. 설정한 에포크 수 만큼 훈련합니다.\n성능 평가 시엔 손실도 같이 보는 경우도 많지만, 이를 확인하는 방법 또한 정확도 계산과 크게 차이가 없습니다. 손실을 구하는 방법은 주석으로 달아놨으니 참고하시면 되겠습니다.\n\nfor epoch in range(n_epochs):\n    \n    model.train() # 모델 훈련\n    # loss_epoch = 0. -&gt; 손실 계산\n    for inputs, labels in dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optim.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optim.step()\n\n        # loss_epoch += loss.item()\n\n    # avg_loss = loss_epoch / len(dls) -&gt; 에포크 당 평균 손실\n\n    model.eval() # 모델 평가\n    with torch.no_grad():\n        total = 0\n        correct = 0\n        for inputs, labels in val_dls:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Epoch: {epoch}, Accuracy on validation set: {100. * correct / total:.2f}')\n\nEpoch: 0, Accuracy on validation set: 96 %\nEpoch: 1, Accuracy on validation set: 97 %\nEpoch: 2, Accuracy on validation set: 98 %\nEpoch: 3, Accuracy on validation set: 98 %\nEpoch: 4, Accuracy on validation set: 97 %\nEpoch: 5, Accuracy on validation set: 98 %\nEpoch: 6, Accuracy on validation set: 98 %\nEpoch: 7, Accuracy on validation set: 98 %\nEpoch: 8, Accuracy on validation set: 98 %\nEpoch: 9, Accuracy on validation set: 98 %\nEpoch: 10, Accuracy on validation set: 98 %\nEpoch: 11, Accuracy on validation set: 98 %\n\n\nKeyboardInterrupt: \n\n\n정확도를 확인하며 적당한 시점에 훈련을 멈추는 것도 괜찮습니다."
  },
  {
    "objectID": "posts/hello/MNIST 숫자 분류.html#결과",
    "href": "posts/hello/MNIST 숫자 분류.html#결과",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "model.eval()\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for inputs, labels in tst_dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total_samples += labels.size(0)\n        total_correct += (predicted == labels).sum().item()\n\naccuracy = 100. * total_correct / total_samples\nprint(f'Accuracy on test set: {accuracy}%')\n\nAccuracy on test set: 99.13%\n\n\n테스트 세트 기준, 이 모델은 99% 정도의 정확도로 MNIST 데이터 셋을 올바르게 분류할 수 있는 것을 확인할 수 있었습니다.\n\n이렇게 비교적 간단한 방법으로 MNIST 손글씨 숫자 데이터 셋을 분류하는 모델을 만들어 봤습니다. 기초부터 만드는 방법도 있지만 ResNet과 같은 모델을 사용하면 시간을 크게 들이지 않고 정확도가 높은 모델을 만들어 낼 수 있습니다.\n다음 시간에도 엔지니어링 관점에서 머신러닝을 잘 활용할 수 있는 포스트로 만나뵙겠습니다.\nCiao!"
  },
  {
    "objectID": "posts/practice/MNIST 숫자 분류.html",
    "href": "posts/practice/MNIST 숫자 분류.html",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "PyTorch에서 불러온 모델을 머신러닝에서 가장 유명한 MNIST 손글씨 숫자 데이터 세트의 숫자를 올바르게 분류하는 모델로 만들어 봅니다.\n\n\nMNIST 손글씨 숫자 데이터 세트를 불러오는 방법에는 여러 가지가 있습니다. 간단한 방법으로는 Keras나 TensorFlow-datasets, Scikit-Learn을 통해 로드하는 방법 등이 있지만, 그래도 가장 쉬운건 역시 직접 다운로드하는 방법입니다.\n여기선 가장 일반적인 방법은 아니지만 torchvision을 통해 데이터 세트를 다운로드하도록 하겠습니다. 참고로 해당 코드에서는 이 후 PyTorch 사용을 위해 데이터 세트는 다운로드와 동시에 텐서로 변환하겠습니다.\n\nfrom torchvision import datasets, transforms\n\ndef load_mnist(root='./data', download=True, transform=transforms.ToTensor()):\n    return (\n        datasets.MNIST(root=root, train=True, download=download, transform=transform),\n        datasets.MNIST(root=root, train=False, download=download, transform=transform)\n    )\n\nmnist_train, mnist_test = load_mnist()\n\n로드된 이미지는 다음과 같이 확인할 수 있습니다. 훈련 세트와 테스트 세트 각 첫 번째 이미지를 확인해 보겠습니다.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nim_trn,lb_trn = mnist_train[0]\nim_tst,lb_tst = mnist_test[0]\n\n# 시각화를 위해 텐서를 28x28 numpy배열로 재변환\nim_trn = im_trn.numpy().reshape(28, 28)\nim_tst = im_tst.numpy().reshape(28, 28)\n\nfig, axs = plt.subplots(1, 2, figsize=(4, 2))\n\naxs[0].imshow(im_trn, cmap='gray')\naxs[0].set_title(lb_trn)\n\naxs[1].imshow(im_tst, cmap='gray')\naxs[1].set_title(lb_tst)\n\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n훈련 세트와 검증 세트를 분리한 후, 훈련 시 데이터들을 불러오는 데이터 로더를 정의해 보겠습니다. 훈련 데이터 세트의 80%를 훈련 세트인 dls로, 나머지를 하나의 에포크가 끝난 후 평가하는 val_dls로 분리합니다. 테스트 데이터 세트는 모델의 마지막 평가를 위해 남겨둡니다. 전부 PyTorch의 모듈을 이용합니다.\n참고로 배치 사이즈는 2의 지수 형태가 일반적이라고 하네요.\n\nfrom torch.utils.data import DataLoader, random_split\n\nbatch_size = 128\ntrain_size = int(0.8 * len(mnist_train))\nval_size = len(mnist_train) - train_size\n\ntrain_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n\ndls = DataLoader(dataset=train_dataset,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 drop_last=True)\n\nval_dls = DataLoader(dataset=val_dataset,\n                     batch_size=batch_size,\n                     shuffle=True,\n                     drop_last=True)\n\ntst_dls = DataLoader(dataset=mnist_test,\n                     batch_size=batch_size,\n                     shuffle=False)\n\n\n\n\n다음은 torchvision의 models 모듈을 통해 모델(‘ResNet-18’)을 정의합니다. 주의할 점은 MNIST 이미지는 흑백(grayscale) 이미지이기 때문에 이미지 입력 레이어의 채널이 3(RGB)이 아닌 1이 되어야 한다는 것입니다. 또한 이 모델은 분류 모델이므로 출력의 종류를 정의하는 선형 레이어를 추가해야합니다.\n\n\nOriginal: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nAfter: Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n미세 조정(Fine-tuning)을 하고 싶다면 모델을 불러오는 과정에서 pretrained 인자를 True로 하고 이미 학습된 모델의 가중치와 편향 등의 매개변수(parameter)를 고정할 수 있게 모델 매개변수의 requires_grad를 False로 바꿔주면 되니 참고하시기 바랍니다.\n\nimport torch\nfrom torchvision import models\n\nn_classes = 10\n\nmodel = models.resnet18(pretrained=False)\n\n# 미세 조정 시 모델의 매개변수를 고정(freeze)\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# 모델의 첫 번째 계층을 수정\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# 출력 계층 수정\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)\n\n\n\n\n손실 함수와 최적화 함수를 정의합니다. 손실 함수는 분류 모델에 쓰이는 Cross Entropy Loss를 사용하고, 최적화 함수엔 Adam을 사용합니다. 스케줄러를 통해 최적화 함수의 속도를 조절합니다. 5 에포크마다 학습률에 감마(0.1)값을 곱합니다.\n\ncriterion = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 15\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n\nGPU 설정을 위해 device 변수를 정의합니다. Nvidia GPU를 사용할 수 있는 환경이라면 ’cuda’를, 그 외에는 ’cpu’를 사용합니다. GPU에서 훈련시키기 위해서는 훈련에 필요한 모든 변수를 GPU로 이동시켜야 합니다. 이를 위해 .to(device)를 사용해 모델을 옮깁니다. 아래 훈련 부분에서도 반복하여 사용됩니다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\n\n\n\n훈련 루프를 작성합니다. train 모드와 eval 모드로 나누어 한 에포크의 훈련이 끝날 때 마다 검증 세트로 성능을 평가합니다. 설정한 에포크 수 만큼 훈련합니다.\n성능 평가 시엔 손실도 같이 보는 경우도 많지만, 이를 확인하는 방법 또한 정확도 계산과 크게 차이가 없습니다. 손실을 구하는 방법은 주석으로 달아놨으니 참고하시면 되겠습니다.\n\nfor epoch in range(n_epochs):\n    \n    model.train() # 모델 훈련\n    # loss_epoch = 0. -&gt; 손실 계산\n    for inputs, labels in dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optim.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optim.step()\n\n        # loss_epoch += loss.item()\n\n    # avg_loss = loss_epoch / len(dls) -&gt; 에포크 당 평균 손실\n\n    model.eval() # 모델 평가\n    with torch.no_grad():\n        total = 0\n        correct = 0\n        for inputs, labels in val_dls:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Epoch: {epoch}, Accuracy on validation set: {100. * correct / total:.2f}')\n\nEpoch: 0, Accuracy on validation set: 96 %\nEpoch: 1, Accuracy on validation set: 97 %\nEpoch: 2, Accuracy on validation set: 98 %\nEpoch: 3, Accuracy on validation set: 98 %\nEpoch: 4, Accuracy on validation set: 97 %\nEpoch: 5, Accuracy on validation set: 98 %\nEpoch: 6, Accuracy on validation set: 98 %\nEpoch: 7, Accuracy on validation set: 98 %\nEpoch: 8, Accuracy on validation set: 98 %\nEpoch: 9, Accuracy on validation set: 98 %\nEpoch: 10, Accuracy on validation set: 98 %\nEpoch: 11, Accuracy on validation set: 98 %\n\n\nKeyboardInterrupt: \n\n\n정확도를 확인하며 적당한 시점에 훈련을 멈추는 것도 괜찮습니다.\n\n\n\n\nmodel.eval()\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for inputs, labels in tst_dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total_samples += labels.size(0)\n        total_correct += (predicted == labels).sum().item()\n\naccuracy = 100. * total_correct / total_samples\nprint(f'Accuracy on test set: {accuracy}%')\n\nAccuracy on test set: 99.13%\n\n\n테스트 세트 기준, 이 모델은 99% 정도의 정확도로 MNIST 데이터 셋을 올바르게 분류할 수 있는 것을 확인할 수 있었습니다.\n\n이렇게 비교적 간단한 방법으로 MNIST 손글씨 숫자 데이터 셋을 분류하는 모델을 만들어 봤습니다. 기초부터 만드는 방법도 있지만 ResNet과 같은 모델을 사용하면 시간을 크게 들이지 않고 정확도가 높은 모델을 만들어 낼 수 있습니다.\n다음 시간에도 엔지니어링 관점에서 머신러닝을 잘 활용할 수 있는 포스트로 만나뵙겠습니다.\nCiao!"
  },
  {
    "objectID": "posts/practice/MNIST 숫자 분류.html#mnist-load",
    "href": "posts/practice/MNIST 숫자 분류.html#mnist-load",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "MNIST 손글씨 숫자 데이터 세트를 불러오는 방법에는 여러 가지가 있습니다. 간단한 방법으로는 Keras나 TensorFlow-datasets, Scikit-Learn을 통해 로드하는 방법 등이 있지만, 그래도 가장 쉬운건 역시 직접 다운로드하는 방법입니다.\n여기선 가장 일반적인 방법은 아니지만 torchvision을 통해 데이터 세트를 다운로드하도록 하겠습니다. 참고로 해당 코드에서는 이 후 PyTorch 사용을 위해 데이터 세트는 다운로드와 동시에 텐서로 변환하겠습니다.\n\nfrom torchvision import datasets, transforms\n\ndef load_mnist(root='./data', download=True, transform=transforms.ToTensor()):\n    return (\n        datasets.MNIST(root=root, train=True, download=download, transform=transform),\n        datasets.MNIST(root=root, train=False, download=download, transform=transform)\n    )\n\nmnist_train, mnist_test = load_mnist()\n\n로드된 이미지는 다음과 같이 확인할 수 있습니다. 훈련 세트와 테스트 세트 각 첫 번째 이미지를 확인해 보겠습니다.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nim_trn,lb_trn = mnist_train[0]\nim_tst,lb_tst = mnist_test[0]\n\n# 시각화를 위해 텐서를 28x28 numpy배열로 재변환\nim_trn = im_trn.numpy().reshape(28, 28)\nim_tst = im_tst.numpy().reshape(28, 28)\n\nfig, axs = plt.subplots(1, 2, figsize=(4, 2))\n\naxs[0].imshow(im_trn, cmap='gray')\naxs[0].set_title(lb_trn)\n\naxs[1].imshow(im_tst, cmap='gray')\naxs[1].set_title(lb_tst)\n\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/practice/MNIST 숫자 분류.html#데이터-로더",
    "href": "posts/practice/MNIST 숫자 분류.html#데이터-로더",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "훈련 세트와 검증 세트를 분리한 후, 훈련 시 데이터들을 불러오는 데이터 로더를 정의해 보겠습니다. 훈련 데이터 세트의 80%를 훈련 세트인 dls로, 나머지를 하나의 에포크가 끝난 후 평가하는 val_dls로 분리합니다. 테스트 데이터 세트는 모델의 마지막 평가를 위해 남겨둡니다. 전부 PyTorch의 모듈을 이용합니다.\n참고로 배치 사이즈는 2의 지수 형태가 일반적이라고 하네요.\n\nfrom torch.utils.data import DataLoader, random_split\n\nbatch_size = 128\ntrain_size = int(0.8 * len(mnist_train))\nval_size = len(mnist_train) - train_size\n\ntrain_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n\ndls = DataLoader(dataset=train_dataset,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 drop_last=True)\n\nval_dls = DataLoader(dataset=val_dataset,\n                     batch_size=batch_size,\n                     shuffle=True,\n                     drop_last=True)\n\ntst_dls = DataLoader(dataset=mnist_test,\n                     batch_size=batch_size,\n                     shuffle=False)"
  },
  {
    "objectID": "posts/practice/MNIST 숫자 분류.html#모델-정의",
    "href": "posts/practice/MNIST 숫자 분류.html#모델-정의",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "다음은 torchvision의 models 모듈을 통해 모델(‘ResNet-18’)을 정의합니다. 주의할 점은 MNIST 이미지는 흑백(grayscale) 이미지이기 때문에 이미지 입력 레이어의 채널이 3(RGB)이 아닌 1이 되어야 한다는 것입니다. 또한 이 모델은 분류 모델이므로 출력의 종류를 정의하는 선형 레이어를 추가해야합니다.\n\n\nOriginal: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nAfter: Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n미세 조정(Fine-tuning)을 하고 싶다면 모델을 불러오는 과정에서 pretrained 인자를 True로 하고 이미 학습된 모델의 가중치와 편향 등의 매개변수(parameter)를 고정할 수 있게 모델 매개변수의 requires_grad를 False로 바꿔주면 되니 참고하시기 바랍니다.\n\nimport torch\nfrom torchvision import models\n\nn_classes = 10\n\nmodel = models.resnet18(pretrained=False)\n\n# 미세 조정 시 모델의 매개변수를 고정(freeze)\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# 모델의 첫 번째 계층을 수정\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# 출력 계층 수정\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)"
  },
  {
    "objectID": "posts/practice/MNIST 숫자 분류.html#손실-함수와-최적화-함수-cuda",
    "href": "posts/practice/MNIST 숫자 분류.html#손실-함수와-최적화-함수-cuda",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "손실 함수와 최적화 함수를 정의합니다. 손실 함수는 분류 모델에 쓰이는 Cross Entropy Loss를 사용하고, 최적화 함수엔 Adam을 사용합니다. 스케줄러를 통해 최적화 함수의 속도를 조절합니다. 5 에포크마다 학습률에 감마(0.1)값을 곱합니다.\n\ncriterion = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 15\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n\nGPU 설정을 위해 device 변수를 정의합니다. Nvidia GPU를 사용할 수 있는 환경이라면 ’cuda’를, 그 외에는 ’cpu’를 사용합니다. GPU에서 훈련시키기 위해서는 훈련에 필요한 모든 변수를 GPU로 이동시켜야 합니다. 이를 위해 .to(device)를 사용해 모델을 옮깁니다. 아래 훈련 부분에서도 반복하여 사용됩니다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)"
  },
  {
    "objectID": "posts/practice/MNIST 숫자 분류.html#훈련",
    "href": "posts/practice/MNIST 숫자 분류.html#훈련",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "훈련 루프를 작성합니다. train 모드와 eval 모드로 나누어 한 에포크의 훈련이 끝날 때 마다 검증 세트로 성능을 평가합니다. 설정한 에포크 수 만큼 훈련합니다.\n성능 평가 시엔 손실도 같이 보는 경우도 많지만, 이를 확인하는 방법 또한 정확도 계산과 크게 차이가 없습니다. 손실을 구하는 방법은 주석으로 달아놨으니 참고하시면 되겠습니다.\n\nfor epoch in range(n_epochs):\n    \n    model.train() # 모델 훈련\n    # loss_epoch = 0. -&gt; 손실 계산\n    for inputs, labels in dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optim.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optim.step()\n\n        # loss_epoch += loss.item()\n\n    # avg_loss = loss_epoch / len(dls) -&gt; 에포크 당 평균 손실\n\n    model.eval() # 모델 평가\n    with torch.no_grad():\n        total = 0\n        correct = 0\n        for inputs, labels in val_dls:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Epoch: {epoch}, Accuracy on validation set: {100. * correct / total:.2f}')\n\nEpoch: 0, Accuracy on validation set: 96 %\nEpoch: 1, Accuracy on validation set: 97 %\nEpoch: 2, Accuracy on validation set: 98 %\nEpoch: 3, Accuracy on validation set: 98 %\nEpoch: 4, Accuracy on validation set: 97 %\nEpoch: 5, Accuracy on validation set: 98 %\nEpoch: 6, Accuracy on validation set: 98 %\nEpoch: 7, Accuracy on validation set: 98 %\nEpoch: 8, Accuracy on validation set: 98 %\nEpoch: 9, Accuracy on validation set: 98 %\nEpoch: 10, Accuracy on validation set: 98 %\nEpoch: 11, Accuracy on validation set: 98 %\n\n\nKeyboardInterrupt: \n\n\n정확도를 확인하며 적당한 시점에 훈련을 멈추는 것도 괜찮습니다."
  },
  {
    "objectID": "posts/practice/MNIST 숫자 분류.html#결과",
    "href": "posts/practice/MNIST 숫자 분류.html#결과",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "model.eval()\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for inputs, labels in tst_dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total_samples += labels.size(0)\n        total_correct += (predicted == labels).sum().item()\n\naccuracy = 100. * total_correct / total_samples\nprint(f'Accuracy on test set: {accuracy}%')\n\nAccuracy on test set: 99.13%\n\n\n테스트 세트 기준, 이 모델은 99% 정도의 정확도로 MNIST 데이터 셋을 올바르게 분류할 수 있는 것을 확인할 수 있었습니다.\n\n이렇게 비교적 간단한 방법으로 MNIST 손글씨 숫자 데이터 셋을 분류하는 모델을 만들어 봤습니다. 기초부터 만드는 방법도 있지만 ResNet과 같은 모델을 사용하면 시간을 크게 들이지 않고 정확도가 높은 모델을 만들어 낼 수 있습니다.\n다음 시간에도 엔지니어링 관점에서 머신러닝을 잘 활용할 수 있는 포스트로 만나뵙겠습니다.\nCiao!"
  },
  {
    "objectID": "posts/practice/MNIST_ResNet.html",
    "href": "posts/practice/MNIST_ResNet.html",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "PyTorch에 존재하는 모델을 MNIST 손글씨 숫자 데이터 세트의 숫자를 올바르게 분류하는 모델로 손쉽게 만들어 봅니다.\n\n\nMNIST 손글씨 숫자 데이터 세트를 불러오는 방법에는 여러 가지가 있습니다. 간단한 방법으로는 Keras나 TensorFlow-datasets, Scikit-Learn을 통해 로드하는 방법 등이 있지만, 그래도 가장 쉬운건 역시 직접 다운로드하는 방법입니다.\n여기선 가장 일반적인 방법은 아니지만 torchvision을 통해 데이터 세트를 다운로드하도록 하겠습니다. 참고로 해당 코드에서는 이 후 PyTorch 사용을 위해 데이터 세트는 다운로드와 동시에 텐서로 변환(transform)하겠습니다.\n\nfrom torchvision import datasets, transforms\n\ndef load_mnist(root='./data', download=True, transform=transforms.ToTensor()):\n    return (\n        datasets.MNIST(root=root, train=True, download=download, transform=transform),\n        datasets.MNIST(root=root, train=False, download=download, transform=transform)\n    )\n\nmnist_train, mnist_test = load_mnist()\n\n로드된 이미지는 다음과 같이 확인할 수 있습니다. 훈련 세트와 테스트 세트 각 첫 번째 이미지를 확인해 보겠습니다.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nim_trn,lb_trn = mnist_train[0]\nim_tst,lb_tst = mnist_test[0]\n\n# 시각화를 위해 텐서를 28x28 numpy배열로 재변환\nim_trn = im_trn.numpy().reshape(28, 28)\nim_tst = im_tst.numpy().reshape(28, 28)\n\nfig, axs = plt.subplots(1, 2, figsize=(4, 2))\n\naxs[0].imshow(im_trn, cmap='gray')\naxs[0].set_title(lb_trn)\n\naxs[1].imshow(im_tst, cmap='gray')\naxs[1].set_title(lb_tst)\n\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n데이터 세트의 시각화 (좌: 훈련 데이터, 우: 테스트 데이터)\n\n\n\n\n\n\n\n훈련 세트와 검증 세트를 분리한 후, 훈련 시 데이터들을 불러오는 데이터 로더를 정의해 보겠습니다. 훈련 데이터 세트의 80%를 훈련 세트인 dls로, 나머지를 하나의 에포크가 끝난 후 평가하는 val_dls로 분리합니다. 테스트 데이터 세트는 모델의 마지막 평가를 위해 남겨둡니다. 모두 PyTorch의 모듈을 이용합니다.\n참고로 배치 사이즈는 2의 지수 형태가 일반적이라고 하네요.\n\nfrom torch.utils.data import DataLoader, random_split\n\nbatch_size = 128\ntrain_size = int(0.8 * len(mnist_train))\nval_size = len(mnist_train) - train_size\n\ntrain_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n\ndls = DataLoader(dataset=train_dataset,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 drop_last=True)\n\nval_dls = DataLoader(dataset=val_dataset,\n                     batch_size=batch_size,\n                     shuffle=True,\n                     drop_last=True)\n\ntst_dls = DataLoader(dataset=mnist_test,\n                     batch_size=batch_size,\n                     shuffle=False)\n\n\n\n\n다음은 torchvision의 models 모듈을 통해 모델(‘ResNet-18’)을 정의합니다. 수정할 부분이 거의 없습니다만, 주의할 점이 있다면 MNIST 이미지는 일반적인 이미지와 다르게 흑백(grayscale) 이미지이기 때문에 이미지 입력 레이어의 채널이 3(RGB)이 아닌 1이 되어야 합니다.\n\n\nOriginal: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nAfter: Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n또한 이 모델은 분류 모델이므로 계층의 마지막 단계에 출력의 종류를 정의하는 선형 레이어를 추가해야합니다.\n\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)\n\n여기선 모델의 구조만을 가지고 오는 것이지만 모델을 처음부터 훈련하는게 아니라 미세 조정(Fine-tuning)을 하고 싶다면, 이미 학습된 모델의 가중치와 편향 등의 매개변수(parameter)를 불러와 고정할 수 있도록 모델을 불러오는 과정에서 pretrained 인자를 True, 모델 매개변수의 requires_grad를 False로 설정하면 됩니다. 주석을 참고하세요.\n\nimport torch\nfrom torchvision import models\n\nn_classes = 10\n\nmodel = models.resnet18(pretrained=False)\n# model = models.resnet18(pretrained=True) -&gt; 매개변수도 같이 로드\n\n# 미세 조정 시 모델의 매개변수를 고정(freeze)\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# 모델의 첫 번째 계층을 수정\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# 출력 계층 수정\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)\n\n\n\n\n손실 함수와 최적화 함수를 정의합니다. 손실 함수는 분류 모델에 쓰이는 Cross Entropy Loss를 사용하고, 최적화 함수엔 Adam을 사용합니다. 스케줄러를 통해 최적화 함수의 속도를 조절합니다. 5 에포크마다 학습률에 감마(0.1)값을 곱합니다.\n\ncriterion = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 15\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n\nGPU 설정을 위해 device 변수를 정의합니다. 해당 변수는 Nvidia GPU를 사용할 수 있는 환경이라면 ’cuda’를, 그 외에는 ’cpu’를 사용합니다. GPU에서 모델을 훈련시키기 위해선 훈련에 필요한 모든 변수를 GPU로 이동시켜야 합니다. 이를 위해 .to(device)를 사용해 모델을 device로 옮깁니다. 이 과정은 아래 훈련 부분에서도 반복하여 사용됩니다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\n\n\n\n훈련 루프를 작성합니다. train 모드와 eval 모드로 나누어 한 에포크의 훈련이 끝날 때 마다 검증 세트로 성능을 평가합니다. 설정한 에포크 수 만큼 훈련합니다.\n성능 평가 시엔 손실을 같이 보는 경우도 많고, 이를 확인하는 방법 또한 정확도 계산과 크게 차이가 없습니다. 주석을 참고하세요.\n\nfor epoch in range(n_epochs):\n    \n    model.train() # 모델 훈련\n    # loss_epoch = 0. -&gt; 손실 계산\n    for inputs, labels in dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optim.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optim.step()\n\n        # loss_epoch += loss.item()\n\n    # avg_loss = loss_epoch / len(dls) -&gt; 에포크 당 평균 손실\n\n    model.eval() # 모델 평가\n    with torch.no_grad():\n        total = 0\n        correct = 0\n        for inputs, labels in val_dls:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Epoch: {epoch}, Accuracy on validation set: {100. * correct / total:.2f}')\n\nEpoch: 0, Accuracy on validation set: 96 %\nEpoch: 1, Accuracy on validation set: 97 %\nEpoch: 2, Accuracy on validation set: 98 %\nEpoch: 3, Accuracy on validation set: 98 %\nEpoch: 4, Accuracy on validation set: 97 %\nEpoch: 5, Accuracy on validation set: 98 %\nEpoch: 6, Accuracy on validation set: 98 %\nEpoch: 7, Accuracy on validation set: 98 %\nEpoch: 8, Accuracy on validation set: 98 %\nEpoch: 9, Accuracy on validation set: 98 %\nEpoch: 10, Accuracy on validation set: 98 %\nEpoch: 11, Accuracy on validation set: 98 %\n\n\nKeyboardInterrupt: \n\n\n정확도를 확인하며 더 이상 개선이 어렵다고 판단되면 적당한 시점에 훈련을 멈추는 것도 좋은 방법입니다.\n\n\n\n\nmodel.eval()\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for inputs, labels in tst_dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total_samples += labels.size(0)\n        total_correct += (predicted == labels).sum().item()\n\naccuracy = 100. * total_correct / total_samples\nprint(f'Accuracy on test set: {accuracy}%')\n\nAccuracy on test set: 99.13%\n\n\n테스트 세트 기준, 이 모델은 99% 정도의 정확도로 MNIST 데이터 셋을 올바르게 분류할 수 있는 것을 확인할 수 있었습니다.\n\n이렇게 비교적 간단한 방법으로 MNIST 손글씨 숫자 데이터 셋을 분류하는 모델을 만들어 봤습니다. ResNet과 같은 구조적으로 증명된 모델을 사용하면 시간을 크게 들이지 않고 정확도가 높은 모델을 만들어 낼 수 있습니다.\n다음 시간에도 엔지니어링 관점에서 머신러닝을 잘 활용할 수 있는 포스트로 만나뵙겠습니다.\nCiao!"
  },
  {
    "objectID": "posts/practice/MNIST_ResNet.html#mnist-load",
    "href": "posts/practice/MNIST_ResNet.html#mnist-load",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "MNIST 손글씨 숫자 데이터 세트를 불러오는 방법에는 여러 가지가 있습니다. 간단한 방법으로는 Keras나 TensorFlow-datasets, Scikit-Learn을 통해 로드하는 방법 등이 있지만, 그래도 가장 쉬운건 역시 직접 다운로드하는 방법입니다.\n여기선 가장 일반적인 방법은 아니지만 torchvision을 통해 데이터 세트를 다운로드하도록 하겠습니다. 참고로 해당 코드에서는 이 후 PyTorch 사용을 위해 데이터 세트는 다운로드와 동시에 텐서로 변환(transform)하겠습니다.\n\nfrom torchvision import datasets, transforms\n\ndef load_mnist(root='./data', download=True, transform=transforms.ToTensor()):\n    return (\n        datasets.MNIST(root=root, train=True, download=download, transform=transform),\n        datasets.MNIST(root=root, train=False, download=download, transform=transform)\n    )\n\nmnist_train, mnist_test = load_mnist()\n\n로드된 이미지는 다음과 같이 확인할 수 있습니다. 훈련 세트와 테스트 세트 각 첫 번째 이미지를 확인해 보겠습니다.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nim_trn,lb_trn = mnist_train[0]\nim_tst,lb_tst = mnist_test[0]\n\n# 시각화를 위해 텐서를 28x28 numpy배열로 재변환\nim_trn = im_trn.numpy().reshape(28, 28)\nim_tst = im_tst.numpy().reshape(28, 28)\n\nfig, axs = plt.subplots(1, 2, figsize=(4, 2))\n\naxs[0].imshow(im_trn, cmap='gray')\naxs[0].set_title(lb_trn)\n\naxs[1].imshow(im_tst, cmap='gray')\naxs[1].set_title(lb_tst)\n\nfor ax in axs:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n데이터 세트의 시각화 (좌: 훈련 데이터, 우: 테스트 데이터)"
  },
  {
    "objectID": "posts/practice/MNIST_ResNet.html#데이터-로더",
    "href": "posts/practice/MNIST_ResNet.html#데이터-로더",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "훈련 세트와 검증 세트를 분리한 후, 훈련 시 데이터들을 불러오는 데이터 로더를 정의해 보겠습니다. 훈련 데이터 세트의 80%를 훈련 세트인 dls로, 나머지를 하나의 에포크가 끝난 후 평가하는 val_dls로 분리합니다. 테스트 데이터 세트는 모델의 마지막 평가를 위해 남겨둡니다. 모두 PyTorch의 모듈을 이용합니다.\n참고로 배치 사이즈는 2의 지수 형태가 일반적이라고 하네요.\n\nfrom torch.utils.data import DataLoader, random_split\n\nbatch_size = 128\ntrain_size = int(0.8 * len(mnist_train))\nval_size = len(mnist_train) - train_size\n\ntrain_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n\ndls = DataLoader(dataset=train_dataset,\n                 batch_size=batch_size,\n                 shuffle=True,\n                 drop_last=True)\n\nval_dls = DataLoader(dataset=val_dataset,\n                     batch_size=batch_size,\n                     shuffle=True,\n                     drop_last=True)\n\ntst_dls = DataLoader(dataset=mnist_test,\n                     batch_size=batch_size,\n                     shuffle=False)"
  },
  {
    "objectID": "posts/practice/MNIST_ResNet.html#모델-정의",
    "href": "posts/practice/MNIST_ResNet.html#모델-정의",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "다음은 torchvision의 models 모듈을 통해 모델(‘ResNet-18’)을 정의합니다. 수정할 부분이 거의 없습니다만, 주의할 점이 있다면 MNIST 이미지는 일반적인 이미지와 다르게 흑백(grayscale) 이미지이기 때문에 이미지 입력 레이어의 채널이 3(RGB)이 아닌 1이 되어야 합니다.\n\n\nOriginal: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nAfter: Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n또한 이 모델은 분류 모델이므로 계층의 마지막 단계에 출력의 종류를 정의하는 선형 레이어를 추가해야합니다.\n\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)\n\n여기선 모델의 구조만을 가지고 오는 것이지만 모델을 처음부터 훈련하는게 아니라 미세 조정(Fine-tuning)을 하고 싶다면, 이미 학습된 모델의 가중치와 편향 등의 매개변수(parameter)를 불러와 고정할 수 있도록 모델을 불러오는 과정에서 pretrained 인자를 True, 모델 매개변수의 requires_grad를 False로 설정하면 됩니다. 주석을 참고하세요.\n\nimport torch\nfrom torchvision import models\n\nn_classes = 10\n\nmodel = models.resnet18(pretrained=False)\n# model = models.resnet18(pretrained=True) -&gt; 매개변수도 같이 로드\n\n# 미세 조정 시 모델의 매개변수를 고정(freeze)\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# 모델의 첫 번째 계층을 수정\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n# 출력 계층 수정\nmodel.fc = torch.nn.Linear(model.fc.in_features, n_classes)"
  },
  {
    "objectID": "posts/practice/MNIST_ResNet.html#손실-함수와-최적화-함수-cuda",
    "href": "posts/practice/MNIST_ResNet.html#손실-함수와-최적화-함수-cuda",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "손실 함수와 최적화 함수를 정의합니다. 손실 함수는 분류 모델에 쓰이는 Cross Entropy Loss를 사용하고, 최적화 함수엔 Adam을 사용합니다. 스케줄러를 통해 최적화 함수의 속도를 조절합니다. 5 에포크마다 학습률에 감마(0.1)값을 곱합니다.\n\ncriterion = torch.nn.CrossEntropyLoss()\noptim = torch.optim.Adam(model.parameters(), lr=0.001)\n\nn_epochs = 15\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n\nGPU 설정을 위해 device 변수를 정의합니다. 해당 변수는 Nvidia GPU를 사용할 수 있는 환경이라면 ’cuda’를, 그 외에는 ’cpu’를 사용합니다. GPU에서 모델을 훈련시키기 위해선 훈련에 필요한 모든 변수를 GPU로 이동시켜야 합니다. 이를 위해 .to(device)를 사용해 모델을 device로 옮깁니다. 이 과정은 아래 훈련 부분에서도 반복하여 사용됩니다.\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)"
  },
  {
    "objectID": "posts/practice/MNIST_ResNet.html#훈련",
    "href": "posts/practice/MNIST_ResNet.html#훈련",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "훈련 루프를 작성합니다. train 모드와 eval 모드로 나누어 한 에포크의 훈련이 끝날 때 마다 검증 세트로 성능을 평가합니다. 설정한 에포크 수 만큼 훈련합니다.\n성능 평가 시엔 손실을 같이 보는 경우도 많고, 이를 확인하는 방법 또한 정확도 계산과 크게 차이가 없습니다. 주석을 참고하세요.\n\nfor epoch in range(n_epochs):\n    \n    model.train() # 모델 훈련\n    # loss_epoch = 0. -&gt; 손실 계산\n    for inputs, labels in dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optim.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optim.step()\n\n        # loss_epoch += loss.item()\n\n    # avg_loss = loss_epoch / len(dls) -&gt; 에포크 당 평균 손실\n\n    model.eval() # 모델 평가\n    with torch.no_grad():\n        total = 0\n        correct = 0\n        for inputs, labels in val_dls:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    print(f'Epoch: {epoch}, Accuracy on validation set: {100. * correct / total:.2f}')\n\nEpoch: 0, Accuracy on validation set: 96 %\nEpoch: 1, Accuracy on validation set: 97 %\nEpoch: 2, Accuracy on validation set: 98 %\nEpoch: 3, Accuracy on validation set: 98 %\nEpoch: 4, Accuracy on validation set: 97 %\nEpoch: 5, Accuracy on validation set: 98 %\nEpoch: 6, Accuracy on validation set: 98 %\nEpoch: 7, Accuracy on validation set: 98 %\nEpoch: 8, Accuracy on validation set: 98 %\nEpoch: 9, Accuracy on validation set: 98 %\nEpoch: 10, Accuracy on validation set: 98 %\nEpoch: 11, Accuracy on validation set: 98 %\n\n\nKeyboardInterrupt: \n\n\n정확도를 확인하며 더 이상 개선이 어렵다고 판단되면 적당한 시점에 훈련을 멈추는 것도 좋은 방법입니다."
  },
  {
    "objectID": "posts/practice/MNIST_ResNet.html#결과",
    "href": "posts/practice/MNIST_ResNet.html#결과",
    "title": "MNIST 숫자 분류",
    "section": "",
    "text": "model.eval()\ntotal_correct = 0\ntotal_samples = 0\n\nwith torch.no_grad():\n    for inputs, labels in tst_dls:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(inputs)\n        _, predicted = torch.max(outputs, 1)\n        total_samples += labels.size(0)\n        total_correct += (predicted == labels).sum().item()\n\naccuracy = 100. * total_correct / total_samples\nprint(f'Accuracy on test set: {accuracy}%')\n\nAccuracy on test set: 99.13%\n\n\n테스트 세트 기준, 이 모델은 99% 정도의 정확도로 MNIST 데이터 셋을 올바르게 분류할 수 있는 것을 확인할 수 있었습니다.\n\n이렇게 비교적 간단한 방법으로 MNIST 손글씨 숫자 데이터 셋을 분류하는 모델을 만들어 봤습니다. ResNet과 같은 구조적으로 증명된 모델을 사용하면 시간을 크게 들이지 않고 정확도가 높은 모델을 만들어 낼 수 있습니다.\n다음 시간에도 엔지니어링 관점에서 머신러닝을 잘 활용할 수 있는 포스트로 만나뵙겠습니다.\nCiao!"
  },
  {
    "objectID": "posts/face-detection_1/face-detection_01.html",
    "href": "posts/face-detection_1/face-detection_01.html",
    "title": "아이돌 얼굴 인식 - 1",
    "section": "",
    "text": "얼굴 인식은 사람의 얼굴 특징을 식별하고 분석하여 개인을 고유하게 식별하는 데 사용되는 기술입니다. 주로 컴퓨터 비전과 패턴 인식 기술을 기반으로 하며, 광학적 또는 사진 기법을 통해 사진이나 비디오에서 얼굴을 감지하고 분석합니다. 보안 시스템, 사용자 인증, 의료, 마케팅 등 다양한 분야에서 활용되는 등, 가능성 또한 광범위합니다.\n이 글에서는 이처럼 여러 측면에서 활용 가능한 얼굴 인식 시스템을 만들고 탐구해봅니다. 이 포스트에서는 K-POP 아이돌 Aespa 4명, NewJeans의 5명의 멤버들 총 9명의 얼굴을 이미지 내에서 탐지하고 인식하는 과정을 소개하겠습니다.\n\n\n시스템에 필요한 데이터를 수집하는 것이 첫 번째 과정입니다. 저는 Pinterest에서 각 인물 당 이미지 약 200장 정도를 저장했습니다. 이 부분은 프로그래밍으로 하지 않고 WFDownloader라는 프로그램을 사용해 이미지 벌크를 보다 쉽게 다운로드 할 수 있었습니다. 이미지 검증 과정이 부족할 관계로 이미지의 명확성을 위해 ’그룹 이름 + 멤버 이름’으로 검색합니다.\n\n\n\n파이썬으로 얼굴을 감지하는 모듈에는 여러 가지가 있고, 이들 모두 쉽게 활용할 수 있습니다. 저는 그 중 MTCNN, Dlib, InsightFace 3가지를 비교해 보았습니다. 3개 모두 파이썬 패키지 외에 별 다른 설치 없이 사용할 수 있습니다.\n\n수집한 이미지 데이터들 중 무작위로 선택한 이미지에서 각각 얼굴을 찾아내고 이를 시각적으로 비교해 보았습니다. \n일부 이미지에서 다음과 같이 성능의 차이를 보입니다. 걸그룹 특성 상, 셀피가 많고 이 경우엔 카메라 각도가 일반적이지 않은 관계로 정교하지 못한 모델은 생각보다 얼굴 감지에 어려움이 있는 듯 했습니다. 따라서 가장 성능이 뛰어난 RetinaFace를 사용하겠습니다.\n사실 RetinaFace는 많은 경우에 현재까지도 정확도와 속도 측면에서 거의 최고 수준의 성능(State-of-the-art)을 달성하고 있습니다. 특히 다양한 크기와 방향의 얼굴을 처리하는 데에 강점을 가지고 있어 시스템의 목적에도 알맞은 모델이기도 합니다. 다만 딥 러닝 모델인 만큼 어느 정도의 부하가 있습니다.\n아래는 RetinaFace를 이용한 얼굴 감지 결과입니다. \n\n\n\n얼굴을 인식하기 위해서는 얼굴 이미지를 학습 시켜야 합니다. 하지만 수집한 이미지 자체로는 이미지에 다른 요소가 많기 때문에 제대로 학습 시킬 수 없습니다.\n이 때문에 얼굴 감지가 필요한 것입니다. 얼굴 감지를 사용하여 얼굴을 중심으로 이미지를 잘라 저장합니다. crop_faces 함수는 20% 정도의 패딩을 주고 얼굴을 중심으로 사각형을 그리는 함수입니다.\n\n이때 얼굴이 2개 이상일 때는 cv2의 Laplacian 메소드를 이용해 감지된 얼굴의 선명도를 비교하고 기준 이상(코드에서는 0.39) 다를 경우 선명한 얼굴만을 잘라내고, 비슷할 경우엔 해당 이미지를 버리는 방식으로 처리했습니다. 이는 사진의 주인공이 흐릿하게 나올 시에는 사진의 주인이 뒤바뀌는 등 부정확하게 처리 될 수도 있다는 점에서 개선이 필요합니다.\n잘라낸 이미지들의 일부는 다음과 같습니다. \n훈련할 모델은 이미지 처리에 특화된 ResNet-50입니다. 이 때문에 훈련을 위해서 행해야 할 전처리 과정은\n\n이미지의 크기를 224 x 224로 조정\n정규화\n데이터의 이미지와 레이블을 (X, Y)로 나눔\n8:1:1 비율로 train, validatoin, test 데이터 셋을 나눔\n\n코드는 아래와 같습니다.\n\n데이터 셋의 크기가 작은 만큼 데이터 증강(Data Augmentation)도 도움이 될 것입니다.\n\n\n\n훈련에 필요한 손실 함수는 Cross Entropy Loss을, 최적화 함수는 Adam을 사용하겠습니다. 훈련 전에 ResNet-50을 로드하고 출력 계층을 카테고리의 길이와 같이 설정하는 것을 잊지 마세요. 제 경우엔 출력 계층의 크기, 곧 len(categories)는 9가 됩니다.\n\n\n100%|██████████| 20/20 [05:29&lt;00:00, 16.49s/it] Validation Loss: 0.8527, Validation Accuracy: 0.8111\n\n코드 실행 결과 Validation 세트 기준으로 80% 정도의 정확도를 확인할 수 있습니다.\n\n\n\nConfusion Matrix를 통해 테스트 셋에서의 정확도를 확인해 보겠습니다. 어디서 혼동이 있었는지 손쉽게 발견할 수 있습니다. \n언뜻 보기에는 모델의 정확도에는 큰 문제가 없어 보입니다. 실제 사진으로 예측해본다면 어떨까요? 예를 들어 얼굴이 두 개 있는 새로운 사진 말입니다.\n이는 1. 모델을 불러오고, 2. 이미지에서 얼굴을 감지한 후, 3. 얼굴 이미지를 잘라 모델이 처리할 수 있는 형태로 transform(정규화, 텐서화 등) 하면, 4. 모델이 해당 이미지의 예측 결과를 출력하는 순서로 진행됩니다.\n\n\n\n두 분 다 지젤 씨가 아니다.\n\n\n확인 결과 이 외의 사진에서도 대부분의 사람을 한 사람으로 예측하는 등 정확도가 크게 떨어지는 것으로 나타났습니다. 본 적 없는 사진에 대해서 낮은 정확도를 보여주는 듯 합니다.\n\n\n\n9명의 이미지를 데이터 셋 내부에선 어느 정도 식별하는 데 성공했지만, 모델의 단순성에 비해 사용자 정의에 대한 유연성이 떨어지기 때문에 제공한 전체적으로 톤 앤 매너가 비슷한 아이돌들의 사진은 구분하기 힘들어 하는 모습을 보입니다. 데이터 셋의 크기와 데이터 처리의 미흡함도 보이지만, 이를 해결할 뾰족한 수가 없는 상황입니다. 따라서 저는 조금 다른 방법을 통해 얼굴 인식 시스템을 만들어 보도록 하겠습니다.\n이어서 얼굴 인식에 최적화된 InsightFace를 통해 얼굴에서 임베딩을 추출하고 이를 통해 유사한 얼굴을 인식하는 조금 더 나은 모델을 만들어 보겠습니다.\n읽어주셔서 감사합니다.\nCiao!\n\n\n\nNBA Face Recognition System using InsightFace - Yongsun Yoon\nInsightFace\nRetinaFace 논문 리뷰"
  },
  {
    "objectID": "posts/face-detection_1/face-detection_01.html#데이터",
    "href": "posts/face-detection_1/face-detection_01.html#데이터",
    "title": "아이돌 얼굴 인식 - 1",
    "section": "",
    "text": "시스템에 필요한 데이터를 수집하는 것이 첫 번째 과정입니다. 저는 Pinterest에서 각 인물 당 이미지 약 200장 정도를 저장했습니다. 이 부분은 프로그래밍으로 하지 않고 WFDownloader라는 프로그램을 사용해 이미지 벌크를 보다 쉽게 다운로드 할 수 있었습니다. 이미지 검증 과정이 부족할 관계로 이미지의 명확성을 위해 ’그룹 이름 + 멤버 이름’으로 검색합니다."
  },
  {
    "objectID": "posts/face-detection_1/face-detection_01.html#얼굴-감지-단계",
    "href": "posts/face-detection_1/face-detection_01.html#얼굴-감지-단계",
    "title": "아이돌 얼굴 인식 - 1",
    "section": "",
    "text": "파이썬으로 얼굴을 감지하는 모듈에는 여러 가지가 있고, 이들 모두 쉽게 활용할 수 있습니다. 저는 그 중 MTCNN, Dlib, InsightFace 3가지를 비교해 보았습니다. 3개 모두 파이썬 패키지 외에 별 다른 설치 없이 사용할 수 있습니다.\n\n수집한 이미지 데이터들 중 무작위로 선택한 이미지에서 각각 얼굴을 찾아내고 이를 시각적으로 비교해 보았습니다. \n일부 이미지에서 다음과 같이 성능의 차이를 보입니다. 걸그룹 특성 상, 셀피가 많고 이 경우엔 카메라 각도가 일반적이지 않은 관계로 정교하지 못한 모델은 생각보다 얼굴 감지에 어려움이 있는 듯 했습니다. 따라서 가장 성능이 뛰어난 RetinaFace를 사용하겠습니다.\n사실 RetinaFace는 많은 경우에 현재까지도 정확도와 속도 측면에서 거의 최고 수준의 성능(State-of-the-art)을 달성하고 있습니다. 특히 다양한 크기와 방향의 얼굴을 처리하는 데에 강점을 가지고 있어 시스템의 목적에도 알맞은 모델이기도 합니다. 다만 딥 러닝 모델인 만큼 어느 정도의 부하가 있습니다.\n아래는 RetinaFace를 이용한 얼굴 감지 결과입니다."
  },
  {
    "objectID": "posts/face-detection_1/face-detection_01.html#참조",
    "href": "posts/face-detection_1/face-detection_01.html#참조",
    "title": "아이돌 얼굴 인식 - 1",
    "section": "",
    "text": "NBA Face Recognition System using InsightFace - Yongsun Yoon\nInsightFace\nRetinaFace 논문 리뷰"
  },
  {
    "objectID": "posts/face-detection_1/face-detection_01.html#이미지-전처리",
    "href": "posts/face-detection_1/face-detection_01.html#이미지-전처리",
    "title": "아이돌 얼굴 인식 - 1",
    "section": "",
    "text": "얼굴을 인식하기 위해서는 얼굴 이미지를 학습 시켜야 합니다. 하지만 수집한 이미지 자체로는 이미지에 다른 요소가 많기 때문에 제대로 학습 시킬 수 없습니다.\n이 때문에 얼굴 감지가 필요한 것입니다. 얼굴 감지를 사용하여 얼굴을 중심으로 이미지를 잘라 저장합니다. crop_faces 함수는 20% 정도의 패딩을 주고 얼굴을 중심으로 사각형을 그리는 함수입니다.\n\n이때 얼굴이 2개 이상일 때는 cv2의 Laplacian 메소드를 이용해 감지된 얼굴의 선명도를 비교하고 기준 이상(코드에서는 0.39) 다를 경우 선명한 얼굴만을 잘라내고, 비슷할 경우엔 해당 이미지를 버리는 방식으로 처리했습니다. 이는 사진의 주인공이 흐릿하게 나올 시에는 사진의 주인이 뒤바뀌는 등 부정확하게 처리 될 수도 있다는 점에서 개선이 필요합니다.\n잘라낸 이미지들의 일부는 다음과 같습니다. \n훈련할 모델은 이미지 처리에 특화된 ResNet-50입니다. 이 때문에 훈련을 위해서 행해야 할 전처리 과정은\n\n이미지의 크기를 224 x 224로 조정\n정규화\n데이터의 이미지와 레이블을 (X, Y)로 나눔\n8:1:1 비율로 train, validatoin, test 데이터 셋을 나눔\n\n코드는 아래와 같습니다.\n\n데이터 셋의 크기가 작은 만큼 데이터 증강(Data Augmentation)도 도움이 될 것입니다."
  },
  {
    "objectID": "posts/face-detection_1/face-detection_01.html#훈련",
    "href": "posts/face-detection_1/face-detection_01.html#훈련",
    "title": "아이돌 얼굴 인식 - 1",
    "section": "",
    "text": "훈련에 필요한 손실 함수는 Cross Entropy Loss을, 최적화 함수는 Adam을 사용하겠습니다. 훈련 전에 ResNet-50을 로드하고 출력 계층을 카테고리의 길이와 같이 설정하는 것을 잊지 마세요. 제 경우엔 출력 계층의 크기, 곧 len(categories)는 9가 됩니다.\n\n\n100%|██████████| 20/20 [05:29&lt;00:00, 16.49s/it] Validation Loss: 0.8527, Validation Accuracy: 0.8111\n\n코드 실행 결과 Validation 세트 기준으로 80% 정도의 정확도를 확인할 수 있습니다."
  },
  {
    "objectID": "posts/face-detection_1/face-detection_01.html#평가",
    "href": "posts/face-detection_1/face-detection_01.html#평가",
    "title": "아이돌 얼굴 인식 - 1",
    "section": "",
    "text": "Confusion Matrix를 통해 테스트 셋에서의 정확도를 확인해 보겠습니다. 어디서 혼동이 있었는지 손쉽게 발견할 수 있습니다. \n언뜻 보기에는 모델의 정확도에는 큰 문제가 없어 보입니다. 실제 사진으로 예측해본다면 어떨까요? 예를 들어 얼굴이 두 개 있는 새로운 사진 말입니다.\n이는 1. 모델을 불러오고, 2. 이미지에서 얼굴을 감지한 후, 3. 얼굴 이미지를 잘라 모델이 처리할 수 있는 형태로 transform(정규화, 텐서화 등) 하면, 4. 모델이 해당 이미지의 예측 결과를 출력하는 순서로 진행됩니다.\n\n\n\n두 분 다 지젤 씨가 아니다.\n\n\n확인 결과 이 외의 사진에서도 대부분의 사람을 한 사람으로 예측하는 등 정확도가 크게 떨어지는 것으로 나타났습니다. 본 적 없는 사진에 대해서 낮은 정확도를 보여주는 듯 합니다."
  },
  {
    "objectID": "posts/face-detection_1/face-detection_01.html#결론",
    "href": "posts/face-detection_1/face-detection_01.html#결론",
    "title": "아이돌 얼굴 인식 - 1",
    "section": "",
    "text": "9명의 이미지를 데이터 셋 내부에선 어느 정도 식별하는 데 성공했지만, 모델의 단순성에 비해 사용자 정의에 대한 유연성이 떨어지기 때문에 제공한 전체적으로 톤 앤 매너가 비슷한 아이돌들의 사진은 구분하기 힘들어 하는 모습을 보입니다. 데이터 셋의 크기와 데이터 처리의 미흡함도 보이지만, 이를 해결할 뾰족한 수가 없는 상황입니다. 따라서 저는 조금 다른 방법을 통해 얼굴 인식 시스템을 만들어 보도록 하겠습니다.\n이어서 얼굴 인식에 최적화된 InsightFace를 통해 얼굴에서 임베딩을 추출하고 이를 통해 유사한 얼굴을 인식하는 조금 더 나은 모델을 만들어 보겠습니다.\n읽어주셔서 감사합니다.\nCiao!"
  },
  {
    "objectID": "posts/face-detection/face-detection_01.html",
    "href": "posts/face-detection/face-detection_01.html",
    "title": "아이돌 얼굴 인식 - ResNet",
    "section": "",
    "text": "얼굴 인식은 사람의 얼굴 특징을 식별하고 분석하여 개인을 고유하게 식별하는 데 사용되는 기술입니다. 주로 컴퓨터 비전과 패턴 인식 기술을 기반으로 하며, 광학적 또는 사진 기법을 통해 사진이나 비디오에서 얼굴을 감지하고 분석합니다. 보안 시스템, 사용자 인증, 의료, 마케팅 등 다양한 분야에서 활용되는 등, 가능성 또한 광범위합니다.\n이 글에서는 이처럼 여러 측면에서 활용 가능한 얼굴 인식 시스템을 만들고 탐구해봅니다. 이 포스트에서는 K-POP 아이돌 Aespa 4명, NewJeans의 5명의 멤버들 총 9명의 얼굴을 이미지 내에서 탐지하고 인식하는 과정을 소개하겠습니다.\n\n\n시스템에 필요한 데이터를 수집하는 것이 첫 번째 과정입니다. 저는 Pinterest에서 각 인물 당 이미지 약 200장 정도를 저장했습니다. 이 부분은 프로그래밍으로 하지 않고 WFDownloader라는 프로그램을 사용해 이미지 벌크를 보다 쉽게 다운로드 할 수 있었습니다. 이미지 검증 과정이 부족할 관계로 이미지의 명확성을 위해 ’그룹 이름 + 멤버 이름’으로 검색합니다.\n수집한 이미지 파일에서 gif, mp4 등의 처리가 불가능한 파일을 제외하고 이미지 파일의 해시를 비교해 중복되는 사진을 제거합니다. 정리된 이미지들은 다음과 같습니다.\n\n\n\n\n\n\n\nFolder Name\nFile Count\n\n\n\n\nAespa Giselle\n     214\n\n\nAespa Karina\n     233\n\n\nAespa Ningning\n     210\n\n\nAespa Winter\n     219\n\n\nNewJeans Danielle\n     218\n\n\nNewJeans Haerin\n     213\n\n\nNewJeans Hanni\n     221\n\n\nNewJeans Hyein\n     203\n\n\nNewJeans Minji\n     221\n\n\n\n\n\n\n파이썬으로 얼굴을 감지하는 모듈에는 여러 가지가 있고, 이들 모두 쉽게 활용할 수 있습니다. 저는 그 중 MTCNN, Dlib, InsightFace 3가지를 비교해 보았습니다. 3개 모두 파이썬 패키지 외에 별 다른 설치 없이 사용할 수 있습니다.\n\n수집한 이미지 데이터들 중 무작위로 선택한 이미지에서 각각 얼굴을 찾아내고 이를 시각적으로 비교해 보았습니다. \n일부 이미지에서 다음과 같이 성능의 차이를 보입니다. 걸그룹 특성 상, 남이 찍어준 사진은 보정이 강하게 들어가며, 셀피의 경우엔 카메라 각도가 일반적이지 않은 (일명 얼빡샷 등) 관계로 정교하지 못한 모델은 생각보다 얼굴 감지에 어려움이 있는 듯 했습니다.\n보다 일반적인 판단을 위해 측정한 모델 별 데이터 세트에서 랜덤으로 사진 100개를 뽑아 얼굴을 인식하는 정도는 다음과 같습니다.\n\n\n\nModel\nScore\n\n\n\n\nMTCNN\n78/100\n\n\nDlib\n81/100\n\n\nInsightFace\n98/100\n\n\n\n여기서는 가장 성능이 뛰어난 RetinaFace를 사용하겠습니다.\n사실 RetinaFace는 많은 경우에 현재까지도 정확도와 속도 측면에서 거의 최고 수준의 성능(State-of-the-art)을 달성하고 있습니다. 특히 다양한 크기와 방향의 얼굴을 처리하는 데에 강점을 가지고 있어 시스템의 목적에도 알맞은 모델이기도 합니다. 다만 딥 러닝 모델인 만큼 어느 정도의 부하가 있습니다.\n아래는 RetinaFace를 이용한 얼굴 감지 결과의 예입니다. \n\n\n\n얼굴을 인식하기 위해서는 얼굴 이미지를 학습 시켜야 합니다. 하지만 수집한 이미지 자체로는 이미지에 다른 요소가 많기 때문에 제대로 학습 시킬 수 없습니다.\n이 때문에 얼굴 감지가 필요한 것입니다. 얼굴 감지를 사용하여 얼굴을 중심으로 이미지를 잘라 저장합니다. crop_faces 함수는 20% 정도의 패딩을 주고 얼굴을 중심으로 사각형을 그리는 함수입니다.\n\n이때 얼굴이 2개 이상일 때는 cv2의 Laplacian 메소드를 이용해 감지된 얼굴의 선명도를 비교하고 기준 이상(코드에서는 0.39) 다를 경우 선명한 얼굴만을 잘라내고, 비슷할 경우엔 해당 이미지를 버리는 방식으로 처리했습니다. 이는 사진의 주인공이 흐릿하게 나올 시에는 사진의 주인이 뒤바뀌는 등 부정확하게 처리 될 수도 있다는 점에서 개선이 필요합니다.\n잘라낸 이미지들의 일부는 다음과 같습니다. \n훈련할 모델은 이미지 처리에 특화된 ResNet-50입니다. 이 때문에 훈련을 위해서 행해야 할 전처리 과정은\n\n이미지의 크기를 224 x 224로 조정\n정규화\n데이터의 이미지와 레이블을 (X, Y)로 나눔\n8:1:1 비율로 train, validatoin, test 데이터 세트을 나눔\n\n코드는 아래와 같습니다.\n\n데이터 세트의 크기가 작은 만큼 데이터 증강(Data Augmentation)도 도움이 될 것입니다.\n\n\n\n훈련에 필요한 손실 함수는 Cross Entropy Loss을, 최적화 함수는 Adam을 사용하겠습니다. 훈련 전에 ResNet-50을 로드하고 출력 계층을 카테고리의 길이와 같이 설정하는 것을 잊지 마세요. 제 경우엔 출력 계층의 크기, 곧 len(categories)는 9가 됩니다.\n\n\n100%|██████████| 20/20 [05:29&lt;00:00, 16.49s/it] Validation Loss: 0.8527, Validation Accuracy: 0.8111\n\n코드 실행 결과 Validation 세트 기준으로 80% 정도의 정확도를 확인할 수 있습니다.\n\n\n\nConfusion Matrix를 통해 Test 세트에서의 정확도를 확인해 보겠습니다. 어디서 혼동이 있었는지 손쉽게 발견할 수 있습니다. \n데이터의 Test 세트를 기준으로 한 평가(정밀도, 재현율, F1 스코어) 결과는 다음과 같습니다.\n\nPrecision: 0.7932904083757975\n\n\nRecall: 0.7847643599427151\n\n\nF1 Score: 0.7826196333837103\n\n언뜻 보기에는 모델의 성능에는 큰 문제가 없어 보입니다. 실제 사진으로 예측해본다면 어떨까요? 예를 들어 얼굴이 두 개 있는 새로운 사진 말입니다.\n이는 1. 모델을 불러오고, 2. 이미지에서 얼굴을 감지한 후, 3. 얼굴 이미지를 잘라 모델이 처리할 수 있는 형태로 transform(정규화, 텐서화 등) 하면, 4. 모델이 해당 이미지의 예측 결과를 출력하는 순서로 진행됩니다.\n\n\n\n두 분 다 지젤 씨가 아니다!\n\n\n확인 결과 이 외의 사진에서도 대부분의 사람을 한 사람으로 예측하는 등 정확도가 크게 떨어지는 것으로 나타났습니다. 본 적 없는 사진에 대해서 낮은 정확도를 보여주는 듯 합니다.\n\n\n\n9명의 이미지를 데이터 세트 내부에선 어느 정도 식별하는 데 성공했지만, 모델의 단순성에 비해 사용자 정의에 대한 유연성이 떨어지기 때문에 전체적으로 톤 앤 매너가 비슷한 아이돌들의 사진은 구분하기 힘들어 하는 모습을 보입니다. 이들이 모두 비슷한 카테고리인 것도 영향을 끼치는 듯 합니다.\n과정 상에서는 얼굴 주변의 패딩을 조금 더 적게 주고 난 후의 이미지로 학습을 시키거나, 얼굴의 방향 등을 유사하게 정렬해 학습 시키는 방향도 성능 개선에 도움이 되었을 것 같습니다. 데이터 세트를 정면의 얼굴로 제한하거나, 데이터 증강을 통해 단순히 이미지의 개수를 늘리는 방법도 고려해 볼 만 합니다. 하지만 결국 개인이 진행하는 프로젝트인 만큼 데이터 세트의 크기와 데이터 처리 방법과 같은 문제들을 근본적으로 해결할 뾰족한 수가 없는 상황입니다. 따라서 이 후에는 다른 방법을 통해 얼굴 인식 시스템을 만들어 보도록 하겠습니다.\n이어서 다음 포스트에서는 얼굴 인식에 최적화된 InsightFace를 통해 얼굴에서 임베딩을 추출하고 코사인 유사도(Cosine Similarity)를 통해 얼굴을 인식하는 모델을 만들어 보겠습니다.\n읽어주셔서 감사합니다.\nCiao!\n\n\n\nInsightFace\nRetinaFace 논문 리뷰"
  },
  {
    "objectID": "posts/face-detection/face-detection_01.html#데이터",
    "href": "posts/face-detection/face-detection_01.html#데이터",
    "title": "아이돌 얼굴 인식 - ResNet",
    "section": "",
    "text": "시스템에 필요한 데이터를 수집하는 것이 첫 번째 과정입니다. 저는 Pinterest에서 각 인물 당 이미지 약 200장 정도를 저장했습니다. 이 부분은 프로그래밍으로 하지 않고 WFDownloader라는 프로그램을 사용해 이미지 벌크를 보다 쉽게 다운로드 할 수 있었습니다. 이미지 검증 과정이 부족할 관계로 이미지의 명확성을 위해 ’그룹 이름 + 멤버 이름’으로 검색합니다.\n수집한 이미지 파일에서 gif, mp4 등의 처리가 불가능한 파일을 제외하고 이미지 파일의 해시를 비교해 중복되는 사진을 제거합니다. 정리된 이미지들은 다음과 같습니다.\n\n\n\n\n\n\n\nFolder Name\nFile Count\n\n\n\n\nAespa Giselle\n     214\n\n\nAespa Karina\n     233\n\n\nAespa Ningning\n     210\n\n\nAespa Winter\n     219\n\n\nNewJeans Danielle\n     218\n\n\nNewJeans Haerin\n     213\n\n\nNewJeans Hanni\n     221\n\n\nNewJeans Hyein\n     203\n\n\nNewJeans Minji\n     221"
  },
  {
    "objectID": "posts/face-detection/face-detection_01.html#얼굴-감지-단계",
    "href": "posts/face-detection/face-detection_01.html#얼굴-감지-단계",
    "title": "아이돌 얼굴 인식 - ResNet",
    "section": "",
    "text": "파이썬으로 얼굴을 감지하는 모듈에는 여러 가지가 있고, 이들 모두 쉽게 활용할 수 있습니다. 저는 그 중 MTCNN, Dlib, InsightFace 3가지를 비교해 보았습니다. 3개 모두 파이썬 패키지 외에 별 다른 설치 없이 사용할 수 있습니다.\n\n수집한 이미지 데이터들 중 무작위로 선택한 이미지에서 각각 얼굴을 찾아내고 이를 시각적으로 비교해 보았습니다. \n일부 이미지에서 다음과 같이 성능의 차이를 보입니다. 걸그룹 특성 상, 남이 찍어준 사진은 보정이 강하게 들어가며, 셀피의 경우엔 카메라 각도가 일반적이지 않은 (일명 얼빡샷 등) 관계로 정교하지 못한 모델은 생각보다 얼굴 감지에 어려움이 있는 듯 했습니다.\n보다 일반적인 판단을 위해 측정한 모델 별 데이터 세트에서 랜덤으로 사진 100개를 뽑아 얼굴을 인식하는 정도는 다음과 같습니다.\n\n\n\nModel\nScore\n\n\n\n\nMTCNN\n78/100\n\n\nDlib\n81/100\n\n\nInsightFace\n98/100\n\n\n\n여기서는 가장 성능이 뛰어난 RetinaFace를 사용하겠습니다.\n사실 RetinaFace는 많은 경우에 현재까지도 정확도와 속도 측면에서 거의 최고 수준의 성능(State-of-the-art)을 달성하고 있습니다. 특히 다양한 크기와 방향의 얼굴을 처리하는 데에 강점을 가지고 있어 시스템의 목적에도 알맞은 모델이기도 합니다. 다만 딥 러닝 모델인 만큼 어느 정도의 부하가 있습니다.\n아래는 RetinaFace를 이용한 얼굴 감지 결과의 예입니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_01.html#이미지-전처리",
    "href": "posts/face-detection/face-detection_01.html#이미지-전처리",
    "title": "아이돌 얼굴 인식 - ResNet",
    "section": "",
    "text": "얼굴을 인식하기 위해서는 얼굴 이미지를 학습 시켜야 합니다. 하지만 수집한 이미지 자체로는 이미지에 다른 요소가 많기 때문에 제대로 학습 시킬 수 없습니다.\n이 때문에 얼굴 감지가 필요한 것입니다. 얼굴 감지를 사용하여 얼굴을 중심으로 이미지를 잘라 저장합니다. crop_faces 함수는 20% 정도의 패딩을 주고 얼굴을 중심으로 사각형을 그리는 함수입니다.\n\n이때 얼굴이 2개 이상일 때는 cv2의 Laplacian 메소드를 이용해 감지된 얼굴의 선명도를 비교하고 기준 이상(코드에서는 0.39) 다를 경우 선명한 얼굴만을 잘라내고, 비슷할 경우엔 해당 이미지를 버리는 방식으로 처리했습니다. 이는 사진의 주인공이 흐릿하게 나올 시에는 사진의 주인이 뒤바뀌는 등 부정확하게 처리 될 수도 있다는 점에서 개선이 필요합니다.\n잘라낸 이미지들의 일부는 다음과 같습니다. \n훈련할 모델은 이미지 처리에 특화된 ResNet-50입니다. 이 때문에 훈련을 위해서 행해야 할 전처리 과정은\n\n이미지의 크기를 224 x 224로 조정\n정규화\n데이터의 이미지와 레이블을 (X, Y)로 나눔\n8:1:1 비율로 train, validatoin, test 데이터 세트을 나눔\n\n코드는 아래와 같습니다.\n\n데이터 세트의 크기가 작은 만큼 데이터 증강(Data Augmentation)도 도움이 될 것입니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_01.html#훈련",
    "href": "posts/face-detection/face-detection_01.html#훈련",
    "title": "아이돌 얼굴 인식 - ResNet",
    "section": "",
    "text": "훈련에 필요한 손실 함수는 Cross Entropy Loss을, 최적화 함수는 Adam을 사용하겠습니다. 훈련 전에 ResNet-50을 로드하고 출력 계층을 카테고리의 길이와 같이 설정하는 것을 잊지 마세요. 제 경우엔 출력 계층의 크기, 곧 len(categories)는 9가 됩니다.\n\n\n100%|██████████| 20/20 [05:29&lt;00:00, 16.49s/it] Validation Loss: 0.8527, Validation Accuracy: 0.8111\n\n코드 실행 결과 Validation 세트 기준으로 80% 정도의 정확도를 확인할 수 있습니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_01.html#평가",
    "href": "posts/face-detection/face-detection_01.html#평가",
    "title": "아이돌 얼굴 인식 - ResNet",
    "section": "",
    "text": "Confusion Matrix를 통해 Test 세트에서의 정확도를 확인해 보겠습니다. 어디서 혼동이 있었는지 손쉽게 발견할 수 있습니다. \n데이터의 Test 세트를 기준으로 한 평가(정밀도, 재현율, F1 스코어) 결과는 다음과 같습니다.\n\nPrecision: 0.7932904083757975\n\n\nRecall: 0.7847643599427151\n\n\nF1 Score: 0.7826196333837103\n\n언뜻 보기에는 모델의 성능에는 큰 문제가 없어 보입니다. 실제 사진으로 예측해본다면 어떨까요? 예를 들어 얼굴이 두 개 있는 새로운 사진 말입니다.\n이는 1. 모델을 불러오고, 2. 이미지에서 얼굴을 감지한 후, 3. 얼굴 이미지를 잘라 모델이 처리할 수 있는 형태로 transform(정규화, 텐서화 등) 하면, 4. 모델이 해당 이미지의 예측 결과를 출력하는 순서로 진행됩니다.\n\n\n\n두 분 다 지젤 씨가 아니다!\n\n\n확인 결과 이 외의 사진에서도 대부분의 사람을 한 사람으로 예측하는 등 정확도가 크게 떨어지는 것으로 나타났습니다. 본 적 없는 사진에 대해서 낮은 정확도를 보여주는 듯 합니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_01.html#결론",
    "href": "posts/face-detection/face-detection_01.html#결론",
    "title": "아이돌 얼굴 인식 - ResNet",
    "section": "",
    "text": "9명의 이미지를 데이터 세트 내부에선 어느 정도 식별하는 데 성공했지만, 모델의 단순성에 비해 사용자 정의에 대한 유연성이 떨어지기 때문에 전체적으로 톤 앤 매너가 비슷한 아이돌들의 사진은 구분하기 힘들어 하는 모습을 보입니다. 이들이 모두 비슷한 카테고리인 것도 영향을 끼치는 듯 합니다.\n과정 상에서는 얼굴 주변의 패딩을 조금 더 적게 주고 난 후의 이미지로 학습을 시키거나, 얼굴의 방향 등을 유사하게 정렬해 학습 시키는 방향도 성능 개선에 도움이 되었을 것 같습니다. 데이터 세트를 정면의 얼굴로 제한하거나, 데이터 증강을 통해 단순히 이미지의 개수를 늘리는 방법도 고려해 볼 만 합니다. 하지만 결국 개인이 진행하는 프로젝트인 만큼 데이터 세트의 크기와 데이터 처리 방법과 같은 문제들을 근본적으로 해결할 뾰족한 수가 없는 상황입니다. 따라서 이 후에는 다른 방법을 통해 얼굴 인식 시스템을 만들어 보도록 하겠습니다.\n이어서 다음 포스트에서는 얼굴 인식에 최적화된 InsightFace를 통해 얼굴에서 임베딩을 추출하고 코사인 유사도(Cosine Similarity)를 통해 얼굴을 인식하는 모델을 만들어 보겠습니다.\n읽어주셔서 감사합니다.\nCiao!"
  },
  {
    "objectID": "posts/face-detection/face-detection_01.html#참조",
    "href": "posts/face-detection/face-detection_01.html#참조",
    "title": "아이돌 얼굴 인식 - ResNet",
    "section": "",
    "text": "InsightFace\nRetinaFace 논문 리뷰"
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html",
    "href": "posts/face-detection/face-detection_02.html",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "",
    "text": "저번 포스트에 이어 이번 포스트에서도 얼굴 인식 모델에 대해 탐구합니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#arcface",
    "href": "posts/face-detection/face-detection_02.html#arcface",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "ArcFace",
    "text": "ArcFace\nArcFace 는 다른 손실 함수와 달리 유클리드 거리(Euclidean Distance)가 아닌 두 벡터 간의 거리인 각거리(Angular Distance), 즉 각도 유사성(Angular Smiliarity)을 측정합니다. 유클리드 거리는 얼굴 데이터와 같은 고차원 공간에서는 의미가 줄어들기 쉽습니다. 반면 각거리는 사진마다 변화가 많은 얼굴 이미지에서 추출한 특징 벡터의 방향성만을 고려하기 때문에, 벡터의 스케일까지 고려하는 유클리드 거리보다 변화의 영향을 덜 받으며 의미를 보존할 가능성이 비교적 높습니다. 이로 인해 접근 방식이 안정적이며, 이는 결과적으로 모델의 일반화 능력을 향상시킵니다.\n또한 ArcFace의 특징 벡터는 정규화 됩니다. 이로 인해 두 벡터의 유사도는 벡터의 스케일 변화 대신 방향의 차이만을 감안하게 됩니다. 아래의 이미지에서 확인할 수 있듯, Softmax의 예측은 하이퍼스피어의 표면에 존재하는 클래스(색)의 범위가 명확하지 않지만, ArcFace는 정규화된 까닭에 반지름이 고정된 하이퍼스피어의 표면에서의 클래스 각각의 위치를 명확하게 구분할 수 있습니다.\n\n\n\n출처: ArcFace: Additive Angular Margin Loss for Deep Face Recognition, [Fig. 3]\n\n\n공식은 아래와 같습니다. 여기서 \\(\\theta_{y_i}\\)는 올바른 클래스의 각도, \\(m\\)은 가산 마진(additive margin) - (일반적으로 0.5 정도의 \\(m\\)은 28.6도 정도의 각도), \\(s\\)는 학습 안정성을 위한 스케일링 팩터입니다. ArcFace가 최적화하는 대상은 특징 벡터와 클래스 중심 사이의 각도입니다. 서로 다른 클래스끼리는 마진 값 덕분에 훈련 과정에서 클래스마다 방향을 중심으로 확실히 분리됩니다.\n\\[L = - \\frac{1}{N} \\sum_{i=1}^N \\log \\frac{e^{s \\cdot (\\cos(\\theta_{y_i} + m))}}{e^{s \\cdot (\\cos(\\theta_{y_i} + m))} + \\sum_{j \\neq y_i} e^{s \\cdot \\cos(\\theta_j)}}\\]\nArcFace는 기존의 얼굴 인식 모델과 비교할 때 눈에 띄는 장점을 보여줍니다. 예를 들어 FaceNet에서 사용하는 삼중항 손실(Triplet Loss)에 비해서, ArcFace는 효율적인 계산을 수행하며 구현하기 쉽고, 샘플 선택에 있어 더 자유롭습니다. 또한 곱 마진을 사용하는 SphereFace와 달리, 합 마진을 사용하기 때문에 비교적 안정적인 성능을 보여줍니다. LFW를 비롯한 벤치마크에서도 99.8% 이상의 정확도를 보이는 등 ArcFace의 뛰어난 성능을 쉽게 확인할 수 있습니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#간단한-구현",
    "href": "posts/face-detection/face-detection_02.html#간단한-구현",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "간단한 구현",
    "text": "간단한 구현\nPyTorch로 ArcFace 손실 함수를 직접 구현해 보도록 하겠습니다. 전체 코드는 다음과 같습니다.\n\n위에서부터 차례차례 살펴보겠습니다. 우선 클래스 초기화 과정에 대해 설명하겠습니다.\n    def __init__(self, in_features, out_features, s=64.0, m=0.5):\n        super(ArcFace, self).__init__()\n        self.in_features = in_features                      \n        self.out_features = out_features                    \n        self.s = s                                          \n        self.m = m                                          \n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\nin_feature의 정확한 이름은 ’Input Feature’로 이를테면 합성곱 신경망(CNN)을 거쳐 출력된 고차원의 특징 벡터, 즉 얼굴 임베딩의 차원을 말합니다. 512차원과 같은 고정된 크기로 출력됩니다.\nout_feature는 클래스의 크기, 즉 구분해야 할 종류를 말합니다.\n하이퍼 파라미터인 s는 스케일링 팩터, m은 마진 값으로 여기선 각각 64와 0.5의 일반적인 기본값으로 설정하며, 이들은 훈련 과정에서 클래스 간 구분을 강화시킵니다.\nweight의 각 행은 특징 공간에서의 클래스를 뜻하며, 값 하나 하나가 공간 내에서의 클래스의 방향을 정의합니다. 훈련 안정성과 수렴을 위해 가중치 값은 균일 분포 값으로 초기화합니다.\n\n이어서 손실을 계산하는 함수 forward에 대해 설명하겠습니다.\n        normalized_input = F.normalize(input)\n        normalized_weight = F.normalize(self.weight)\n\nInput Feature와 가중치(weight)를 정규화합니다. 정규화는 ArcFace의 중요한 특징으로 앞서 나왔던 내용입니다.\n\n        cos_theta = F.linear(normalized_input, normalized_weight)\n\n        theta = torch.acos(cos_theta.clamp(-1.0 + 1e-7, 1.0 - 1e-7))\n        target_logit = torch.cos(theta + self.m)\n\nInput과 가중치의 F.linear(Dot Product, \\(\\cdot\\))를 계산합니다. 다시 말해 해당 식은 Input Feature의 방향과 각 클래스의 중심 사이의 각도(\\(\\theta\\))에 대한 코사인을 구하게 됩니다. 자세한 내용은 Dot Product와 코사인의 관계를 참고하세요.\n\\(\\theta\\)값을 도출해내기 위해 acos(아크코사인)을 적용합니다. clamp 함수는 [-1. 1] 범위의 입력에만 정의된 아크코사인 함수를 위해 사용됩니다.\n\\(\\theta\\)값에 마진을 더해 다시 코사인으로 변환한 값을 target_logit으로 정의합니다.\n\n        one_hot = torch.zeros_like(cos_theta)\n        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n\n올바른 클래스를 찾기 위해 원-핫 인코딩을 생성합니다.\n\n        output = (one_hot * target_logit) + ((1.0 - one_hot) * cos_theta)\n\n        output *= self.s\n        return F.cross_entropy(output, label)\n\n마진 값이 들어간 코사인 값인 target_logit을 올바른 클래스에만 적용시킵니다. 일치하는 클래스에 한해 마진을 더해 코사인 유사성을 올리는 것 입니다.\n스케일링 계수를 곱하고, 마지막으로 교차 엔트로피 손실을 계산합니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#평가",
    "href": "posts/face-detection/face-detection_02.html#평가",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "평가",
    "text": "평가\n위의 코드는 각각 클래스마다 30개의 임베딩을 추출하고, 15개는 얼굴 인식 임베딩(known_embeddings), 나머지 15개는 임베딩의 정확도를 평가하기 위한 임베딩(unknown_embeddings)으로 저장합니다. 평가 방식은 제가 참고한 글에서도 이미 언급했듯이, Flatten(평탄화)된 임베딩보다는 임베딩 Average(평균)와 비교하여 인식하는 편이 성능이 낫습니다.\n이유는 일반화 능력 향상에 있습니다. Average 방식은 같은 얼굴의 여러 임베딩을 평균 계산하여 노이즈와 이상치 확률을 감소시킵니다. 사진에서 볼 수 있는 같지만 변형된 얼굴을 일반화하므로 정확도 향상에 도움이 됩니다. 반면 Flatten 방식이 유리한 점은 계산이 간단하다는 것입니다. 임베딩이 제대로 추출했다는 가정하에는 좋은 결과를 보여줄 수 있습니다.\n따라서 저는 Average 방식으로만 임베딩을 평가하고 테스트에 사용하겠습니다. Threshold에 따른 unknown_embedding의 평가 결과입니다. 그래프에서 Accuracy는 얼굴 일치 여부를 Coverage는 얼굴 식별 여부를 뜻합니다.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a range of threshold values\nthreshold_values = np.arange(0.1, 1.0, 0.05)\n\n# Lists to store the averages for each threshold\nacc_averages = []\ncov_averages = []\n\nfor threshold in threshold_values:\n    pred_names = search_average(known_embeddings, known_names, unknown_embeddings, threshold=threshold)\n    acc_average, cov_average = evaluate(unknown_names, pred_names)\n    \n    acc_averages.append(acc_average)\n    cov_averages.append(cov_average)\n\n# Plotting\nplt.figure(figsize=(10, 6))\n\nplt.plot(threshold_values, acc_averages, label='Accuracy Average', marker='o')\nplt.plot(threshold_values, cov_averages, label='Coverage Average', marker='x')\n\nplt.xlabel('Threshold')\nplt.ylabel('Average Value')\nplt.title('Evaluation Metrics by Threshold')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nEmbedding Graph\n\n\n0.4를 지나기 전에 두 선이 교차하는 구간이, 0.6을 지나자 두 지수 모두 급격하게 감소하는 구간이 관찰됩니다. 교차하는 구간인 0.3 정도를 최적의 Threshold로 정하겠습니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#임베딩-구하기",
    "href": "posts/face-detection/face-detection_02.html#임베딩-구하기",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "임베딩 구하기",
    "text": "임베딩 구하기\n일단 InsightFace에서 모델을 가져와야 합니다. InsightFace의 깃허브 페이지에서도 모델을 다운받고 직접 모델을 로드할 수 있으나, 여기서는 편의성을 위해 이전 포스트와 같이 파이썬 패키지를 통해 다운하고 로드하겠습니다. 모델의 크기를 직접 지정해야 하는 등의 세부 설정이 필요하다면 링크를 참조하시기 바랍니다.\n\n임베딩 추출에 적합하지 않은 경우는 두 가지 입니다.\n\n사진 내의 인식되는 얼굴이 여러 개일 경우\nDetection Score가 수준 이하일 때\n\n다음은 코드를 통해 알아본 임의의 이미지 100개 중 부적합한 이미지들의 예시입니다. 얼굴 불인식의 기준은 Detection Score=0.63 미만입니다.\n\n\nCode\nplt.figure(figsize=(20, 20))\nd_faces = 0\n\nfile_paths = [Path(category)/Path(filename) for category in categories for filename in os.listdir(raw_dir/category)]\nrandom.shuffle(file_paths)\n\n# 임의의 이미지 100개를 평가\nfor i in range(100):    \n    file_path = raw_dir/file_paths[i]\n    img = cv2.imread(str(file_path))\n\n    faces = model.get(img)\n    # 1. 이미지 내 얼굴이 1개가 아닐 경우\n    if len(faces) != 1:\n        d_faces += 1\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        plt.subplot(5, 10, d_faces)\n        plt.axis('off')\n        plt.imshow(img_rgb)\n        plt.title(f'{len(faces)=}')\n        continue\n\n    for face in faces:\n        # 2. 얼굴 인식 점수가 낮을 경우\n        if face.det_score &lt; 0.63:    \n            d_faces += 1\n            (x1, y1, x2, y2) = map(int, face.bbox)\n            face_img = img[y1:y2, x1:x2]\n            if face_img.size &gt; 0:   \n                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            plt.subplot(5, 10, d_faces)\n            plt.axis('off')\n            plt.imshow(img_rgb)\n            plt.title(f'{face.det_score:.2f}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nExamples\n\n\n이들의 임베딩을 구해서 저장해 보겠습니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#다른-손실-함수와의-비교",
    "href": "posts/face-detection/face-detection_02.html#다른-손실-함수와의-비교",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "다른 손실 함수와의 비교",
    "text": "다른 손실 함수와의 비교\n얼굴 인식에 주로 쓰이는 마진 기반의 손실 함수들인 SphereFace, CosFace, ArcFace를 표로 비교해 보았습니다. 셋 모두 기존의 Softmax보다는 성능이 훨씬 뛰어나며, 주요 벤치마크에서의 성능은 SphereFace &lt; CosFace &lt;= ArcFace 입니다.\n\n\n\n\n\n\n\n\n\n손실 함수\nSphereFace\nCosFace (AM-Softmax)\nArcFace\n\n\n\n\n마진의 종류\n곱 각도(Multiplicative Angular)\n가산 코사인(Additive Cosine)\n가산 각도(Additive Angular)\n\n\n작동 공간\n각 공간\n코사인 공간\n각 공간\n\n\n최적화 방식\n간접적\n간접적\n직접적\n\n\n훈련 안정성\n마진의 곱 계산으로 인해 불안정 가능성\n대체적으로 안정적\n대체적으로 안정적\n\n\n해석 가능성\n보통\n보통\n높음 (직접 각도를 수정)\n\n\n구현 난이도\n어려움\n쉬움\n쉬움\n\n\n성능\n좋음\n매우 좋음\n우수함\n\n\n주요 특징\n각도 마진을 최초로 도입\n간단하고 효과적임\n측지(Geodesic) 거리를 직접 최적화\n\n\n단점\n수렴 가능성\n기하학적인 해석이 어려움\n마진 선택의 중요성"
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#간단한-시각화",
    "href": "posts/face-detection/face-detection_02.html#간단한-시각화",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "간단한 시각화",
    "text": "간단한 시각화\n\n\n\nFeature Space\n\n\n이미지는 검은색, 살구색, 붉은색 3개의 클래스를 생성해 ArcFace로 분류한 하이퍼 스피어를 2차원 평면에서 보여주고 있습니다. 가로축은 class 0과의 코사인 유사도, 세로축은 class 1과의 코사인 유사도를 뜻합니다.\n검은색 클래스는 class 0과 class 1 그 어디에도 속하지 않는다고 볼 수 있는데, 이는 두 클래스와의 유사도 모두 -1 구간에 집중되어 분포하고 있는 클래스들의 색이 검은색인 것으로 확인할 수 있습니다. 이는 모두 같은 크기의 호를 가진 하이퍼 스피어의 클래스들이 각도 공간에서 뚜렷한 차이를 보여주며 ArcFace가 세 클래스를 학습하고 명확하게 분리했음을 보여줍니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#구현",
    "href": "posts/face-detection/face-detection_02.html#구현",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "구현",
    "text": "구현\nPyTorch로 ArcFace 손실 함수를 직접 구현해 보도록 하겠습니다. 전체 코드는 다음과 같습니다.\n\n위에서부터 차례차례 살펴보겠습니다. 우선 클래스 초기화 과정에 대해 설명하겠습니다.\n    def __init__(self, in_features, out_features, s=64.0, m=0.5):\n        super(ArcFace, self).__init__()\n        self.in_features = in_features                      \n        self.out_features = out_features                    \n        self.s = s                                          \n        self.m = m                                          \n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\nin_feature의 정확한 이름은 ’Input Feature’로 이를테면 합성곱 신경망(CNN)을 거쳐 출력된 고차원의 특징 벡터, 즉 얼굴 임베딩 벡터의 차원의 수를 말합니다. 512(차원)과 같은 고정된 크기로 출력됩니다.\nout_feature는 클래스의 크기, 즉 구분해야 할 종류를 말합니다.\n하이퍼 파라미터인 s는 스케일링 팩터, m은 마진 값으로 여기선 각각 64와 0.5의 일반적인 기본값으로 설정하며, 이들은 훈련 과정에서 클래스 간 구분을 강화시킵니다.\nweight의 각 행은 특징 공간에서의 클래스를 뜻하며, 값 하나 하나가 공간 내에서의 클래스의 방향을 정의합니다. 훈련 안정성과 수렴을 위해 가중치 값은 균일 분포 값으로 초기화합니다.\n\n이어서 손실을 계산하는 함수 forward에 대해 설명하겠습니다.\n        normalized_input = F.normalize(input)\n        normalized_weight = F.normalize(self.weight)\n\nInput Feature와 가중치(weight)를 정규화합니다. 정규화는 ArcFace의 중요한 특징으로 앞서 나왔던 내용입니다.\n\n        cos_theta = F.linear(normalized_input, normalized_weight)\n\n        theta = torch.acos(cos_theta.clamp(-1.0 + 1e-7, 1.0 - 1e-7))\n        target_logit = torch.cos(theta + self.m)\n\nInput과 가중치의 F.linear(Dot Product, \\(\\cdot\\))를 계산합니다. 다시 말해 해당 식은 Input Feature의 방향과 각 클래스의 중심 사이의 각도(\\(\\theta\\))에 대한 코사인을 구하게 됩니다. 자세한 내용은 Dot Product와 코사인의 관계를 참고하세요.\n\\(\\theta\\)값을 도출해내기 위해 acos(아크코사인)을 적용합니다. clamp 함수는 [-1. 1] 범위의 입력에만 정의된 아크코사인 함수를 위해 사용됩니다.\n\\(\\theta\\)값에 마진을 더해 다시 코사인으로 변환한 값을 target_logit으로 정의합니다.\n\n        one_hot = torch.zeros_like(cos_theta)\n        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n\n올바른 클래스를 찾기 위해 원-핫 인코딩을 생성합니다.\n\n        output = (one_hot * target_logit) + ((1.0 - one_hot) * cos_theta)\n\n        output *= self.s\n        return F.cross_entropy(output, label)\n\n마진 값이 들어간 코사인 값인 target_logit을 올바른 클래스에만 적용시킵니다. 일치하는 클래스에 한해 마진을 더해 코사인 유사성을 올리는 것 입니다.\n스케일링 계수를 곱하고, 마지막으로 교차 엔트로피 손실을 계산합니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#테스트",
    "href": "posts/face-detection/face-detection_02.html#테스트",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "테스트",
    "text": "테스트\n먼저 데이터 셋 내의 랜덤한 이미지를 대상으로 인식한 결과입니다.\n\n\n\nTest Case 1\n\n\n이 둘은 일치하는 인물에 0.4 이상의 인식률을 보이며, 인식할 수 있는 모든 인물 내에서 큰 차이 또한 나타냅니다.\n\n\n\nTest Case 2\n\n\n\n두 사람의 얼굴이 포착됐지만, 클래스에 존재하지 않는 한 사람은 0에 가까운 유사도를 보이기 때문에 인식되지 않습니다. 인식된 연예인은 역시 0.4 이상 일치하는 결과를 보여줍니다.\n역시 0.5 이상의 일치율을 보입니다.\n유사도가 가장 높은 인물은 정확하지만 임계 값을 넘지 못해 unknown으로 출력됩니다.\n정확하게 출력합니다.\n\n다음은 데이터 셋 외부의 이미지로, 다른 걸그룹 혹은 클래스에 존재하지 않는 사람과의 이미지에서 테스트 해보겠습니다.\n 뉴진스 다섯 멤버는 모두 꽤 높은 정도로 인식하는 모습입니다.\n 이 이미지에선 클래스 내부에서 찾을 수 없는 인물 또한 잘 구분해 올바르게 매치합니다.\n 다만 여기선 다른 사람을 인식하는 모습입니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#시각화",
    "href": "posts/face-detection/face-detection_02.html#시각화",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "시각화",
    "text": "시각화\n\n\n\nFeature Space\n\n\n이미지는 검은색, 살구색, 붉은색 3개의 클래스를 생성해 ArcFace로 분류한 하이퍼 스피어를 2차원 평면에서 보여주고 있습니다. 가로축은 class 0과의 코사인 유사도, 세로축은 class 1과의 코사인 유사도를 뜻합니다.\n검은색 클래스는 class 0과 class 1 그 어디에도 속하지 않는다고 볼 수 있는데, 이는 두 클래스와의 유사도 모두 -1 구간에 집중되어 분포하고 있는 클래스들의 색이 검은색인 것으로 확인할 수 있습니다. 이는 모두 같은 크기의 호를 가진 하이퍼 스피어의 클래스들이 각도 공간에서 뚜렷한 차이를 보여주며 ArcFace가 세 클래스를 학습하고 명확하게 분리했음을 보여줍니다."
  },
  {
    "objectID": "posts/face-detection/face-detection_02.html#결론",
    "href": "posts/face-detection/face-detection_02.html#결론",
    "title": "아이돌 얼굴 인식 - InsightFace",
    "section": "결론",
    "text": "결론\nInsightFace의 임베딩을 사용하면 저번 시간에 시도했던 이미지 인식 모델보다 정확도가 상당히 높아진다는 것을 알 수 있었습니다. 또한 InsightFace가 사용하는 얼굴 인식에 특화된 손실 함수 ArcFace가 왜 정확도가 높은지, 여기에 쓰이는 코사인 유사도와 구현 등에 대해서 자세히 알아보았습니다.\n여기서 더 정확도를 높이기 위해서는, 이미지 선별을 통해 좋은 임베딩을 추출하고, 임베딩의 수를 늘려서 일반화 정도를 높이는 방안이 고려됩니다. 현재의 적은 클래스를 감안하면, 클래스의 수를 늘린 후에도 정확도를 높일 수 있을지 실험이 더 필요하겠습니다.\n얼굴 인식에 대한 글은 이만 줄이고 다음 시간에는 조금 더 재밌는 포스트로 찾아 오겠습니다.\nCiao!"
  }
]